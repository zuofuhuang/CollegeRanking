[
["index.html", "454 Bayesian Statistics Project - College Ranking Chapter1 Preface", " 454 Bayesian Statistics Project - College Ranking Zuofu Huang and Kavya Shetty Chapter1 Preface Hello! Welcome to the bookdown of our MATH 454 Bayesian Statistics Project! We are having a lot of fun doing this project, and you are always welcome to come back at any point for our progress! Credit to xkcd: https://xkcd.com/1236/ "],
["motivation.html", "Chapter2 Motivation 2.1 Why this? 2.2 A snapshot", " Chapter2 Motivation 2.1 Why this? Why have Princeton and Williams topped the US News and World Report ranking charts for so long? What factors are considered when we create college rankings? Though reductionary, college rankings are often what parents and prospective students consider in some way when approaching their college search process. Rankings do not often change a great deal and there are many aspects that play into how a ranking is constructed based on what the compiler deems important. In our report, we aim to investigate how these rankings are constructed and eventually provide a specialized tool for students to tailor a college’s “rank” to their own set of criteria. 2.2 A snapshot In our project, we aim to look at the factors that play into college rankings and use our findings to create a specialized ranking tool for students. Factors like region, setting, size, admissions rate, diversity, and standardized testing scores play into the score a given school will receive. "],
["data-source.html", "Chapter3 Data source 3.1 Data that makes our work possible 3.2 Our data dictionary 3.3 Changes to datasets", " Chapter3 Data source 3.1 Data that makes our work possible Our dataset was collected from College Scorecard, which is a tool from the College Affordability and Transparency Center. The data is held on data.gov from the Department of Education and maintained by Brian Fu. The last update was September 29, 2017 and our years of interest include 2014-2017. This dataset includes all the aforementioned variables and provides a profile for each school. In order to cross-reference these with ranking data, we scraped data from Andre G. Reiter’s U.S. News &amp; World Report Historical Liberal Arts College and University Rankings datasets, which he compiled from both print and digital records. There were some data missing and we filled that with information compiled on the website Public University Honors. The dataset was called “Average U.S. News Rankings for 123 Universities: 2012-2019” and was posted September 18, 2016 and was most recently updated on September 10, 2018 to include the 2019 rankings. It’s only possible to do what we do because of the awesome data! http://andyreiter.com/datasets/ (A base of University and Liberal Arts ranking datasets) https://publicuniversityhonors.com/2016/09/18/average-u-s-news-rankings-for-126-universities-2010-1017/ (Where we have access to missing University rankings that allow us to input manually) https://catalog.data.gov/dataset/college-scorecard (Three big datasets that founds our analysis) https://collegescorecard.ed.gov/data/documentation/ (Big dataset dictionary) 3.2 Our data dictionary Variable Name Description Institution Name (INSTNM) name of college/university Region (REGION) region-wise (e.g. Minnesota is in the Midwest) Setting (LOCALE) setting (large city, small town, etc.) Size of School (UGDS) number of undergraduate degree-seeking students Admission Rate (ADM_RATE) college’s admission rate SAT Scores (SAT_AVG) combined SAT average by year, interchangeable with SAT Racial Diversity (UGDS_WHITE) racial diversity by percent of white students Average Cost Per Year (COSTT4_A) average cost of attendance per year 3.3 Changes to datasets To ensure the consistency among datasets, we made a few necessary changes to the original datasets. For University ranking datasets: We collected and manually added rankings of 60 universities between 2012 and 2019. For Liberal Arts College ranking datasets: To avoid repetition, we changed Westminster College in Pennsylvania to “Westminster College-PA”; changed Wheaton College in Massachusetts to “Wheaton College-MA”. We deleted US Military Academy, US Air Force Academy, US Naval Academy because the infomration is not disclosed to the public and thus not available for viewing. We deleted Grove City College and Principia College because their information were not available in the mega datasets. We made slight changes on a few colleges, such as “St. Norbert College” to “Saint Norbert College” in order to match with the name in mega datasets. For Mega datasets: To avoid repetition, we changed Westminster College in Pennsylvania to “Westminster College-PA”; changed Wheaton College in Massachusetts to “Wheaton College-MA”. We renamed the unranked St. John’s College by its geographic location; renamed the two unranked Union Colleges by their geographic locations. "],
["data-cleaning.html", "Chapter4 Data cleaning 4.1 What did we do?", " Chapter4 Data cleaning 4.1 What did we do? If you don’t want to look at all the lengthy work, Kavya and Zuofu are excited to talk through the process with you! library(readxl) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(ggplot2) library(rjags) ## Loading required package: coda ## Linked to JAGS 4.3.0 ## Loaded modules: basemod,bugs library(forcats) library(gridExtra) ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine 4.1.1 Load data, select and rename columns myData1617 &lt;- read.csv(file = &quot;/Users/kevinhuang/Desktop/Spring 2019/MATH 454/454 Project/CollegeScorecard_Raw_Data/MERGED2016_17_PP.csv&quot;) myData1617_sub &lt;- myData1617 %&gt;% select(INSTNM,REGION,LOCALE,UGDS,ADM_RATE,ACTCMMID,SAT_AVG,UGDS_WHITE,COSTT4_A) names(myData1617_sub) &lt;- c(&quot;INSTNM&quot;, &quot;REGION&quot;,&quot;LOCALE&quot;,&quot;UGDS_1617&quot;,&quot;ADM_RATE_1617&quot;,&quot;ACTCMMID_1617&quot;,&quot;SAT_AVG_1617&quot;,&quot;UGDS_WHITE_1617&quot;,&quot;COSTT4_A_1617&quot;) myData1516 &lt;- read.csv(file = &quot;/Users/kevinhuang/Desktop/Spring 2019/MATH 454/454 Project/CollegeScorecard_Raw_Data/MERGED2015_16_PP.csv&quot;) myData1516_sub &lt;- myData1516 %&gt;% select(INSTNM,UGDS,ADM_RATE,ACTCMMID,SAT_AVG,UGDS_WHITE,COSTT4_A) names(myData1516_sub) &lt;- c(&quot;INSTNM&quot;,&quot;UGDS_1516&quot;,&quot;ADM_RATE_1516&quot;,&quot;ACTCMMID_1516&quot;,&quot;SAT_AVG_1516&quot;,&quot;UGDS_WHITE_1516&quot;,&quot;COSTT4_A_1516&quot;) myData1415 &lt;- read.csv(file = &quot;/Users/kevinhuang/Desktop/Spring 2019/MATH 454/454 Project/CollegeScorecard_Raw_Data/MERGED2014_15_PP.csv&quot;) myData1415_sub &lt;- myData1415 %&gt;% select(INSTNM,UGDS,ADM_RATE,ACTCMMID,SAT_AVG,UGDS_WHITE,COSTT4_A) names(myData1415_sub) &lt;- c(&quot;INSTNM&quot;,&quot;UGDS_1415&quot;,&quot;ADM_RATE_1415&quot;,&quot;ACTCMMID_1415&quot;,&quot;SAT_AVG_1415&quot;,&quot;UGDS_WHITE_1415&quot;,&quot;COSTT4_A_1415&quot;) USNewsUniversity &lt;- read_excel(&quot;/Users/kevinhuang/Desktop/Spring 2019/MATH 454/454 Project/USNewsUniversity.xlsx&quot;) newUniversity &lt;- USNewsUniversity %&gt;% select(c(UniversityName, State, Y2019, Y2018, Y2017, Y2016, Y2015, Y2014, Y2013, Y2012)) %&gt;% na.omit() USNewsLiberalArts &lt;- read_excel(&quot;/Users/kevinhuang/Desktop/Spring 2019/MATH 454/454 Project/USNewsLiberalArts.xlsx&quot;) newLiberalArts &lt;- USNewsLiberalArts %&gt;% select(c(CollegeName, State, Y2019, Y2018, Y2017, Y2016, Y2015, Y2014, Y2013, Y2012)) %&gt;% na.omit() 4.1.2 Filter schools that have a rank college &lt;- rep(NA,149) for (i in 1:149){ college[i] = newLiberalArts[i,]$CollegeName } university &lt;- rep(NA,120) for (i in 1:120){ university[i] = newUniversity[i,]$UniversityName } schools &lt;- c(college, university) # Run tests to make sure that datasets are consistent: check_1_Data1617 &lt;- myData1617_sub %&gt;% filter(INSTNM %in% college) dim(check_1_Data1617) check_3_Data1415 &lt;- myData1415_sub %&gt;% filter(INSTNM %in% college) dim(check_3_Data1415) check_2_Data1516 &lt;- myData1516_sub %&gt;% filter(INSTNM %in% college) dim(check_2_Data1516) # Identify class of each variable, in preparation for transformations. class(myData1617_sub$UGDS_1617) class(myData1617_sub$ADM_RATE_1617) class(myData1617_sub$ACTCMMID_1617) class(myData1617_sub$SAT_AVG_1617) class(myData1617_sub$UGDS_WHITE_1617) class(myData1617_sub$COSTT4_A_1617) class(myData1415_sub$INSTNM) myData1415_sub_characterCollege &lt;- myData1415_sub %&gt;% filter(INSTNM %in% college) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1415 = as.numeric(as.character(UGDS_1415))) %&gt;% mutate(UGDS_WHITE_1415 = as.numeric(as.character(UGDS_WHITE_1415))) %&gt;% mutate(SAT_AVG_1415 = as.numeric(as.character(SAT_AVG_1415))) %&gt;% mutate(ACTCMMID_1415 = as.numeric(as.character(ACTCMMID_1415))) %&gt;% mutate(ADM_RATE_1415 = as.numeric(as.character(ADM_RATE_1415))) %&gt;% mutate(COSTT4_A_1415 = as.numeric(as.character(COSTT4_A_1415))) ## Warning in evalq(as.numeric(as.character(SAT_AVG_1415)), &lt;environment&gt;): 强 ## 制改变过程中产生了NA ## Warning in evalq(as.numeric(as.character(ACTCMMID_1415)), &lt;environment&gt;): ## 强制改变过程中产生了NA ## Warning in evalq(as.numeric(as.character(ADM_RATE_1415)), &lt;environment&gt;): ## 强制改变过程中产生了NA myData1516_sub_characterCollege &lt;- myData1516_sub %&gt;% filter(INSTNM %in% college) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1516 = as.numeric(as.character(UGDS_1516))) %&gt;% mutate(UGDS_WHITE_1516 = as.numeric(as.character(UGDS_WHITE_1516))) %&gt;% mutate(SAT_AVG_1516 = as.numeric(as.character(SAT_AVG_1516))) %&gt;% mutate(ACTCMMID_1516 = as.numeric(as.character(ACTCMMID_1516))) %&gt;% mutate(ADM_RATE_1516 = as.numeric(as.character(ADM_RATE_1516))) %&gt;% mutate(COSTT4_A_1516 = as.numeric(as.character(COSTT4_A_1516))) ## Warning in evalq(as.numeric(as.character(SAT_AVG_1516)), &lt;environment&gt;): 强 ## 制改变过程中产生了NA ## Warning in evalq(as.numeric(as.character(ACTCMMID_1516)), &lt;environment&gt;): ## 强制改变过程中产生了NA myData1617_sub_characterCollege &lt;- myData1617_sub %&gt;% filter(INSTNM %in% college) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1617 = as.numeric(as.character(UGDS_1617))) %&gt;% mutate(UGDS_WHITE_1617 = as.numeric(as.character(UGDS_WHITE_1617))) %&gt;% mutate(SAT_AVG_1617 = as.numeric(as.character(SAT_AVG_1617))) %&gt;% mutate(ACTCMMID_1617 = as.numeric(as.character(ACTCMMID_1617))) %&gt;% mutate(ADM_RATE_1617 = as.numeric(as.character(ADM_RATE_1617))) %&gt;% mutate(COSTT4_A_1617 = as.numeric(as.character(COSTT4_A_1617))) ## Warning in evalq(as.numeric(as.character(SAT_AVG_1617)), &lt;environment&gt;): 强 ## 制改变过程中产生了NA ## Warning in evalq(as.numeric(as.character(ACTCMMID_1617)), &lt;environment&gt;): ## 强制改变过程中产生了NA # To recheck that all three datasets are the same with regard to college names. dim(myData1415_sub_characterCollege) dim(myData1516_sub_characterCollege) dim(myData1617_sub_characterCollege) 4.1.3 Join datasets fullLiberalArts &lt;- newLiberalArts %&gt;% full_join(myData1415_sub_characterCollege, by = c(&quot;CollegeName&quot; = &quot;INSTNM&quot;)) %&gt;% full_join(myData1516_sub_characterCollege, by = c(&quot;CollegeName&quot; = &quot;INSTNM&quot;)) %&gt;% full_join(myData1617_sub_characterCollege, by = c(&quot;CollegeName&quot; = &quot;INSTNM&quot;)) dim(fullLiberalArts) ## [1] 175 30 full_LiberalArts &lt;- fullLiberalArts[!duplicated(fullLiberalArts$CollegeName), ] dim(full_LiberalArts) # After we removed the repetitive entries. ## [1] 149 30 myData1415_sub_characterUniversity &lt;- myData1415_sub %&gt;% filter(INSTNM %in% university) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1415 = as.numeric(as.character(UGDS_1415))) %&gt;% mutate(UGDS_WHITE_1415 = as.numeric(as.character(UGDS_WHITE_1415))) %&gt;% mutate(SAT_AVG_1415 = as.numeric(as.character(SAT_AVG_1415))) %&gt;% mutate(ACTCMMID_1415 = as.numeric(as.character(ACTCMMID_1415))) %&gt;% mutate(ADM_RATE_1415 = as.numeric(as.character(ADM_RATE_1415))) %&gt;% mutate(COSTT4_A_1415 = as.numeric(as.character(COSTT4_A_1415))) ## Warning in evalq(as.numeric(as.character(SAT_AVG_1415)), &lt;environment&gt;): 强 ## 制改变过程中产生了NA ## Warning in evalq(as.numeric(as.character(ACTCMMID_1415)), &lt;environment&gt;): ## 强制改变过程中产生了NA myData1516_sub_characterUniversity &lt;- myData1516_sub %&gt;% filter(INSTNM %in% university) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1516 = as.numeric(as.character(UGDS_1516))) %&gt;% mutate(UGDS_WHITE_1516 = as.numeric(as.character(UGDS_WHITE_1516))) %&gt;% mutate(SAT_AVG_1516 = as.numeric(as.character(SAT_AVG_1516))) %&gt;% mutate(ACTCMMID_1516 = as.numeric(as.character(ACTCMMID_1516))) %&gt;% mutate(ADM_RATE_1516 = as.numeric(as.character(ADM_RATE_1516))) %&gt;% mutate(COSTT4_A_1516 = as.numeric(as.character(COSTT4_A_1516))) ## Warning in evalq(as.numeric(as.character(SAT_AVG_1516)), &lt;environment&gt;): 强 ## 制改变过程中产生了NA ## Warning in evalq(as.numeric(as.character(ACTCMMID_1516)), &lt;environment&gt;): ## 强制改变过程中产生了NA myData1617_sub_characterUniversity &lt;- myData1617_sub %&gt;% filter(INSTNM %in% university) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1617 = as.numeric(as.character(UGDS_1617))) %&gt;% mutate(UGDS_WHITE_1617 = as.numeric(as.character(UGDS_WHITE_1617))) %&gt;% mutate(SAT_AVG_1617 = as.numeric(as.character(SAT_AVG_1617))) %&gt;% mutate(ACTCMMID_1617 = as.numeric(as.character(ACTCMMID_1617))) %&gt;% mutate(ADM_RATE_1617 = as.numeric(as.character(ADM_RATE_1617))) %&gt;% mutate(COSTT4_A_1617 = as.numeric(as.character(COSTT4_A_1617))) ## Warning in evalq(as.numeric(as.character(SAT_AVG_1617)), &lt;environment&gt;): 强 ## 制改变过程中产生了NA ## Warning in evalq(as.numeric(as.character(ACTCMMID_1617)), &lt;environment&gt;): ## 强制改变过程中产生了NA fullUniversity &lt;- newUniversity %&gt;% full_join(myData1415_sub_characterUniversity, by = c(&quot;UniversityName&quot; = &quot;INSTNM&quot;)) %&gt;% full_join(myData1516_sub_characterUniversity, by = c(&quot;UniversityName&quot; = &quot;INSTNM&quot;)) %&gt;% full_join(myData1617_sub_characterUniversity, by = c(&quot;UniversityName&quot; = &quot;INSTNM&quot;)) dim(fullUniversity) ## [1] 120 30 "],
["visualization.html", "Chapter5 Visualization 5.1 1st &amp; 2nd Peeks 5.2 3rd Peek 5.3 4th Peek", " Chapter5 Visualization Intuitively, what’s going on? 5.1 1st &amp; 2nd Peeks In our first and second peeks, we are interested in geographical locations of universities and colleges. It is surprising to learn that students, when choosing schools, give much weight to a school’s geographical location. We investigate two variables that contribute to prospective students’ choice, giving side-by-side plots that reflect how the variables vary between universities and liberal arts colleges. REGION_collapse &lt;- fct_collapse(as.character(full_LiberalArts$REGION), NewEngland = &quot;1&quot;, MidEast = &quot;2&quot;, GreatLakes = &quot;3&quot;, Plains = &quot;4&quot;, Southeast = &quot;5&quot;, Southwest = &quot;6&quot;, RockyMountains = &quot;7&quot;, FarWest = &quot;8&quot;) full_LiberalArts$REGION_collapse &lt;- REGION_collapse LOCALE_collapse_lac1 &lt;- fct_collapse(full_LiberalArts$LOCALE, City = c(&quot;11&quot;,&quot;12&quot;,&quot;13&quot;), Suburb = c(&quot;21&quot;,&quot;22&quot;,&quot;23&quot;), Town = c(&quot;31&quot;,&quot;32&quot;,&quot;33&quot;), Rural = c(&quot;41&quot;,&quot;42&quot;,&quot;43&quot;)) LOCALE_collapse_lac2 &lt;- fct_collapse(full_LiberalArts$LOCALE, LargeCity = &quot;11&quot;, MidsizeCity = &quot;12&quot;, SmallCity = &quot;13&quot;, LargeSuburb = &quot;21&quot;, MidsizeSuburb = &quot;22&quot;, SmallSuburb = &quot;23&quot;, FringeTown = &quot;31&quot;, DistantTown = &quot;32&quot;, RemoteTown = &quot;33&quot;, FringeRural = &quot;41&quot;, DistantRural = &quot;42&quot;, RemoteTown = &quot;43&quot;) full_LiberalArts$LOCALE_collapse_lac1 &lt;- LOCALE_collapse_lac1 full_LiberalArts$LOCALE_collapse_lac2 &lt;- LOCALE_collapse_lac2 REGION_collapse &lt;- fct_collapse(as.character(fullUniversity$REGION), NewEngland = &quot;1&quot;, MidEast = &quot;2&quot;, GreatLakes = &quot;3&quot;, Plains = &quot;4&quot;, Southeast = &quot;5&quot;, Southwest = &quot;6&quot;, RockyMountains = &quot;7&quot;, FarWest = &quot;8&quot;) fullUniversity$REGION_collapse &lt;- REGION_collapse LOCALE_collapse &lt;- fct_collapse(fullUniversity$LOCALE, City = c(&quot;11&quot;,&quot;12&quot;,&quot;13&quot;), Suburb = c(&quot;21&quot;,&quot;22&quot;,&quot;23&quot;), Town = c(&quot;31&quot;,&quot;32&quot;,&quot;33&quot;), Rural = c(&quot;41&quot;,&quot;42&quot;,&quot;43&quot;)) LOCALE_collapse2 &lt;- fct_collapse(fullUniversity$LOCALE, LargeCity = &quot;11&quot;, MidsizeCity = &quot;12&quot;, SmallCity = &quot;13&quot;, LargeSuburb = &quot;21&quot;, MidsizeSuburb = &quot;22&quot;, SmallSuburb = &quot;23&quot;, FringeTown = &quot;31&quot;, DistantTown = &quot;32&quot;, RemoteTown = &quot;33&quot;, FringeRural = &quot;41&quot;, DistantRural = &quot;42&quot;, RemoteTown = &quot;43&quot;) fullUniversity$LOCALE_collapse &lt;- LOCALE_collapse fullUniversity$LOCALE_collapse2 &lt;- LOCALE_collapse2 g1 &lt;- ggplot(fullUniversity, aes(x = REGION_collapse, fill = LOCALE_collapse)) + geom_bar() + xlab(&quot;Region&quot;) + scale_fill_discrete(name = &quot;Breakdown&quot;) + ylab(&quot;Count&quot;) + ggtitle(&quot;Region of Universities, broken down by location&quot;) g2 &lt;- ggplot(full_LiberalArts, aes(x = REGION_collapse, fill = LOCALE_collapse_lac1)) + geom_bar() + xlab(&quot;Region&quot;) + scale_fill_discrete(name = &quot;Breakdown&quot;) + ylab(&quot;Count&quot;) + ggtitle(&quot;Region of Liberal Arts Colleges, broken down by location&quot;) grid.arrange(g1, g2, nrow = 2) g3 &lt;- ggplot(fullUniversity, aes(x = LOCALE_collapse, fill = LOCALE_collapse2)) + geom_bar() + xlab(&quot;Geographical Location&quot;) + scale_fill_discrete(name = &quot;Breakdown&quot;) + ylab(&quot;Count&quot;) + ggtitle(&quot;Location of Universities, broken down by size&quot;) + theme(plot.title = element_text(size = 10)) g4 &lt;- ggplot(full_LiberalArts, aes(x = LOCALE_collapse_lac1, fill = LOCALE_collapse_lac2)) + geom_bar() + xlab(&quot;Geographical Location&quot;) + scale_fill_discrete(name = &quot;Breakdown&quot;) + ylab(&quot;Count&quot;) + ggtitle(&quot;Location of Liberal Arts Colleges, broken down by size&quot;) + theme(plot.title = element_text(size = 9.5)) grid.arrange(g3, g4, nrow = 1) Similar as Checkpoint 3, for the latter two visualizations we will solely focus on visualizations and inference from the university dataset. Inference of liberal arts colleges is indentical to that of universities; we will work on combining two inferences together when building the Shiny App for students. 5.2 3rd Peek This is a two-variable visualization of Ranking by SAT score. ggplot(fullUniversity, aes(x = SAT_AVG_1617, y = as.numeric(Y2018))) + geom_point() + geom_smooth(method = &quot;lm&quot;) + xlim(1000,1600) + ylim(1,150) + xlab(&quot;Average SAT Score&quot;) + ylab(&quot;School Ranking 2018&quot;) ## Warning: Removed 7 rows containing non-finite values (stat_smooth). ## Warning: Removed 7 rows containing missing values (geom_point). ## Warning: Removed 8 rows containing missing values (geom_smooth). 5.3 4th Peek On our fourth peek of the dataset, we plan to look into the relationship between the cost of each school and diversity factors (namely, the precentage of white students and size of the student body). fullUniversity$UGDS_WHITE_1617_categ = cut(fullUniversity$UGDS_WHITE_1617, c(0,0.2,0.4,0.6,0.8,1)) ggplot(fullUniversity, aes(x = UGDS_1617, y = UGDS_WHITE_1617, color = COSTT4_A_1617)) + geom_point() + xlab(&quot;Size of Undergradute Student Body&quot;) + scale_fill_discrete(name = &quot;Cost for Year 2017&quot;) + ylab(&quot;Percentage of White Students&quot;) + ggtitle(&quot;Economic and Racial Diversity&quot;) We didn’t see an observable relationship between cost and the percentage of white students; we did observe that universities of a smaller size tend to correlate with a much larger cost. "],
["bayesian-models-part-1.html", "Chapter6 Bayesian Models Part 1 6.1 Model 0 6.2 Model 1 6.3 Model 2 6.4 Future steps", " Chapter6 Bayesian Models Part 1 How will we simulate what we want to know? HINT: Start SIMPLE 6.1 Model 0 Ranking by one year of SAT score: with intuition from \\(\\text{hist}(\\sqrt{1/rgamma(10000,a,b)})\\). 6.1.1 Building the model # DEFINE the model university_model_0 &lt;- &quot;model{ for(i in 1:length(y)) { # Data model y[i] ~ dnorm(beta0 + beta1 * x[i], tau0) } # Priors for theta beta0 ~ dnorm(300,1/250000) beta1 ~ dnorm(0, 1/100) tau0 ~ dgamma(7,4000) }&quot; # COMPILE the model model_data0 &lt;- data.frame(y = fullUniversity$Y2018, x = fullUniversity$SAT_AVG_1617) model_data0 &lt;- na.omit(model_data0) university_jags_0 &lt;- jags.model(textConnection(university_model_0), data = list(y = model_data0$y, x = model_data0$x), inits = list(.RNG.name = &quot;base::Wichmann-Hill&quot;, .RNG.seed = 454)) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 114 ## Unobserved stochastic nodes: 3 ## Total graph size: 440 ## ## Initializing model # SIMULATE the model university_sim_0 &lt;- coda.samples(university_jags_0, variable.names = c(&quot;beta0&quot;, &quot;beta1&quot;, &quot;tau0&quot;), n.iter = 10000) # STORE the chains in a data frame university_chains_0 &lt;- data.frame(university_sim_0[[1]]) 6.1.2 Model summary summary(university_sim_0) ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## beta0 403.411563 3.227e+01 3.227e-01 4.714e+00 ## beta1 -0.263423 2.472e-02 2.472e-04 3.617e-03 ## tau0 0.002421 3.279e-04 3.279e-06 1.303e-05 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## beta0 355.59133 390.793008 405.743679 421.198553 447.191343 ## beta1 -0.29699 -0.277127 -0.265219 -0.253713 -0.226779 ## tau0 0.00182 0.002212 0.002415 0.002628 0.003062 6.1.3 Posterior inference For an unknown university with a mean student SAT score of 1450 (e.g. Bvictor University), we could predict its ranking from our rjags simulation. university_chains_0 &lt;- university_chains_0 %&gt;% mutate(ranking_new = rnorm(10000, mean = beta0 + beta1*1450, sd = (1/tau0)^(1/2))) university_chains_0 %&gt;% summarize(quantile(ranking_new,0.025),quantile(ranking_new,0.975)) ## quantile(ranking_new, 0.025) quantile(ranking_new, 0.975) ## 1 -19.52518 61.70322 A \\(95\\%\\) credible interval is \\((-20,62)\\). Due to restrictions of this one-predictor model, we expect to see an improvement in later models. 6.2 Model 1 Ranking by three years of SAT score. By analysis, the three predictors are multicollinear, meaning that this may not be a significantly better model than Model 0. 6.2.1 Building the model university_model_1 &lt;- &quot;model{ # Data: observations for(i in 1:length(y)) { y[i] ~ dnorm(beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x3[i], tau_big) } # Data: subjects beta0 ~ dnorm(0,1/250000) beta1 ~ dnorm(0,100) beta2 ~ dnorm(0,100) beta3 ~ dnorm(0,100) tau_big ~ dgamma(7,1000) }&quot; # COMPILE y &lt;- fullUniversity$Y2018 model_data1 &lt;- as.data.frame(cbind(y, x1 = fullUniversity$SAT_AVG_1415, x2 = fullUniversity$SAT_AVG_1516, x3 = fullUniversity$SAT_AVG_1617)) model_data1 &lt;- na.omit(model_data1) university_jags_1 &lt;- jags.model(textConnection(university_model_1), data = list(y = model_data1$y, x1 = model_data1$x1, x2 = model_data1$x2, x3 = model_data1$x3), inits=list(.RNG.name = &quot;base::Wichmann-Hill&quot;, .RNG.seed = 454)) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 114 ## Unobserved stochastic nodes: 5 ## Total graph size: 876 ## ## Initializing model # SIMULATE the model university_sim_1 &lt;- coda.samples(university_jags_1, variable.names = c(&quot;beta0&quot;,&quot;beta1&quot;,&quot;beta2&quot;,&quot;beta3&quot;,&quot;tau_big&quot;), n.iter = 10000) # STORE the chains in a data frame university_chains_1 &lt;- data.frame(university_sim_1[[1]]) 6.2.2 Model summary summary(university_sim_1) ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## beta0 409.864989 3.553e+01 3.553e-01 5.783e+00 ## beta1 0.001373 1.078e-01 1.078e-03 1.120e-01 ## beta2 -0.179656 7.938e-02 7.938e-04 4.684e-02 ## beta3 -0.090932 7.417e-02 7.417e-04 5.086e-02 ## tau_big 0.002781 4.008e-04 4.008e-06 2.714e-05 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## beta0 345.977738 400.686962 414.807439 427.055384 447.427911 ## beta1 -0.224077 -0.074142 0.033869 0.084271 0.127709 ## beta2 -0.304552 -0.247353 -0.181364 -0.127231 -0.006660 ## beta3 -0.212426 -0.151206 -0.106569 -0.025275 0.034454 ## tau_big 0.002051 0.002538 0.002778 0.003029 0.003556 6.2.3 Posterior inference For an unknown university with three years’ mean student SAT score of 1450, 1440, 1420 (e.g. Cvictor University), we could predict its ranking from our rjags simulation. university_chains_1 &lt;- university_chains_1 %&gt;% mutate(ranking_new = rnorm(10000, mean = beta0 + beta1*1450 + beta2*1440 +beta3*1420, sd = (1/tau_big)^(1/2))) university_chains_1 %&gt;% summarize(quantile(ranking_new,0.025),quantile(ranking_new,0.975)) ## quantile(ranking_new, 0.025) quantile(ranking_new, 0.975) ## 1 -14.50881 62.8556 A \\(95\\%\\) credible interval is \\((-14,64)\\). The \\(95\\%\\) credible interval in the new model does a better job at eliminating negative rankings, which are impossible in reality. 6.3 Model 2 2018 U.S. News Ranking by average SAT score and admissions rate of Year 2017. 6.3.1 Building the model 6.3.1.1 Step 1 A linear model provides some intuition about how the priors might be constructed. summary(lm(fullUniversity$Y2018 ~ fullUniversity$SAT_AVG_1617 + fullUniversity$ADM_RATE_1617)) ## ## Call: ## lm(formula = fullUniversity$Y2018 ~ fullUniversity$SAT_AVG_1617 + ## fullUniversity$ADM_RATE_1617) ## ## Residuals: ## Min 1Q Median 3Q Max ## -78.978 -9.142 -1.986 8.865 43.380 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 172.5262 45.1297 3.823 0.000218 *** ## fullUniversity$SAT_AVG_1617 -0.1150 0.0301 -3.822 0.000219 *** ## fullUniversity$ADM_RATE_1617 84.8854 14.8094 5.732 8.66e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.42 on 111 degrees of freedom ## (6 observations deleted due to missingness) ## Multiple R-squared: 0.7858, Adjusted R-squared: 0.7819 ## F-statistic: 203.6 on 2 and 111 DF, p-value: &lt; 2.2e-16 6.3.1.2 Step 2 university_model_2 &lt;- &quot;model{ # Data: observations for(i in 1:length(y)) { y[i] ~ dnorm(beta0[i] + beta1[i]*x1[i] + beta2[i]*x2[i], tau_big[i]) # Data: subjects beta0[i] ~ dnorm(b0, tau0) beta1[i] ~ dnorm(b1,tau1) beta2[i] ~ dnorm(b2,tau2) tau_big[i] ~ dgamma(s,r) } # Hyperpriors b0 ~ dnorm(180,1/4000) tau0 ~ dnorm(30, 1/9) b1 ~ dnorm(-0.1,1000) tau1 ~ dnorm(1000,1000) b2 ~ dnorm(80,1/100) tau2 ~ dnorm(10,1) s ~ dnorm(7,1) r ~ dnorm(10000, 1/10000) }&quot; # COMPILE y &lt;- fullUniversity$Y2018 model_data2 &lt;- as.data.frame(cbind(y, x1 = fullUniversity$SAT_AVG_1617, x2 = fullUniversity$ADM_RATE_1617)) model_data2 &lt;- na.omit(model_data2) university_jags_2 &lt;- jags.model(textConnection(university_model_2), data = list(y = model_data2$y, x1 = model_data2$x1, x2 = model_data2$x2), inits=list(.RNG.name = &quot;base::Wichmann-Hill&quot;, .RNG.seed = 454)) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 114 ## Unobserved stochastic nodes: 464 ## Total graph size: 1165 ## ## Initializing model # SIMULATE the model university_sim_2 &lt;- coda.samples(university_jags_2, variable.names = c(&quot;b0&quot;,&quot;tau0&quot;,&quot;b1&quot;,&quot;tau1&quot;,&quot;b2&quot;,&quot;tau2&quot;,&quot;s&quot;,&quot;r&quot;), n.iter = 10000) # STORE the chains in a data frame university_chains_2 &lt;- data.frame(university_sim_2[[1]]) 6.3.2 Model summary summary(university_sim_2) ## ## Iterations = 1001:11000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## b0 181.003 0.866642 8.666e-03 0.4845152 ## b1 -0.122 0.003986 3.986e-05 0.0001962 ## b2 86.978 2.748797 2.749e-02 2.0167560 ## r 9978.437 98.495695 9.850e-01 1.3555384 ## s 9.367 0.854744 8.547e-03 0.0428560 ## tau0 29.903 3.051727 3.052e-02 0.0544687 ## tau1 1000.000 0.031393 3.139e-04 0.0003934 ## tau2 10.021 1.000127 1.000e-02 0.0179231 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## b0 179.2258 180.2942 181.163 181.6543 182.5227 ## b1 -0.1298 -0.1247 -0.122 -0.1193 -0.1143 ## b2 80.6675 85.0648 87.714 89.2203 90.5414 ## r 9784.6197 9912.1376 9978.523 10044.9089 10170.4126 ## s 7.7114 8.7880 9.345 9.9331 11.0906 ## tau0 23.8816 27.8340 29.879 31.9392 35.9038 ## tau1 999.9366 999.9792 1000.000 1000.0208 1000.0615 ## tau2 8.0599 9.3477 10.030 10.6819 12.0006 6.3.3 Posterior inference For an unknown university with student mean SAT score of 1450 and an admission rate of \\(30\\%\\) (e.g. Dvictor University), we could predict its ranking from our rjags simulation. university_chains_2 &lt;- university_chains_2 %&gt;% mutate(beta0_new = rnorm(10000,b0,(1/tau0)^(1/2))) %&gt;% mutate(beta1_new = rnorm(10000,b1,(1/tau1)^(1/2))) %&gt;% mutate(beta2_new = rnorm(10000,b2,(1/tau2)^(1/2))) %&gt;% mutate(tau_big_new = rgamma(10000,s,r)) %&gt;% mutate(ranking_new = rnorm(10000, mean = beta0_new + beta1_new * 1450 + beta2_new * 0.3, sd = (1/tau_big_new)^(1/2))) university_chains_2 %&gt;% summarize(quantile(ranking_new,0.025),quantile(ranking_new,0.975)) ## quantile(ranking_new, 0.025) quantile(ranking_new, 0.975) ## 1 -83.75616 145.3569 A \\(95\\%\\) credible interval is \\((-86,142)\\). The \\(95\\%\\) credible interval does a poor job at eliminating negative rankings or making an accurate prediction. We propose that this may be due to the variability of the priors and hyperpriors set in our Hierarchical Model. 6.4 Future steps Next, we plan to incorporate more predictive variables of ranking \\(y_i\\). Since some predictors we choose are correlated, we will also emphasize on reflecting that in our models. "],
["bayesian-models-part-2.html", "Chapter7 Bayesian Models Part 2", " Chapter7 Bayesian Models Part 2 How will we simulate what we want to know? HINT: After you start SIMPLE, everything suddenly goes really complicated. "]
]
