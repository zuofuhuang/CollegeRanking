[
["index.html", "454 Bayesian Statistics Project - College Ranking Chapter 1 Preface", " 454 Bayesian Statistics Project - College Ranking Zuofu Huang and Kavya Shetty, Spring 2019 Macalester College Chapter 1 Preface Hello! Welcome to the bookdown of our MATH 454 Bayesian Statistics Project! We are having a lot of fun doing this project, and you are always welcome to come back at any point for our progress! Credit to xkcd: https://xkcd.com/1236/ "],
["motivation.html", "Chapter 2 Motivation 2.1 Why this? 2.2 A snapshot", " Chapter 2 Motivation 2.1 Why this? Why have Princeton and Williams topped the US News and World Report ranking charts for so long? What factors are considered when we create college rankings? Though reductionary, college rankings are often what parents and prospective students consider in some way when approaching their college search process. Rankings do not often change a great deal and there are many aspects that play into how a ranking is constructed based on what the compiler deems important. In our report, we aim to investigate how these rankings are constructed and eventually provide a specialized tool for students to tailor a college’s “rank” to their own set of criteria. 2.2 A snapshot In our project, we aim to look at the factors that play into college rankings and use our findings to create a specialized ranking tool for students. Factors like region, setting, size, admissions rate, diversity, and standardized testing scores play into the score a given school will receive. "],
["data-source.html", "Chapter 3 Data source 3.1 Data that makes our work possible 3.2 Our data dictionary 3.3 Changes to datasets", " Chapter 3 Data source 3.1 Data that makes our work possible Our dataset was collected from College Scorecard, which is a tool from the College Affordability and Transparency Center. The data is held on data.gov from the Department of Education and maintained by Brian Fu. The last update was September 29, 2017 and our years of interest include 2014-2017. This dataset includes all the aforementioned variables and provides a profile for each school. In order to cross-reference these with ranking data, we scraped data from Andre G. Reiter’s U.S. News &amp; World Report Historical Liberal Arts College and University Rankings datasets, which he compiled from both print and digital records. There were some data missing and we filled that with information compiled on the website Public University Honors. The dataset was called “Average U.S. News Rankings for 123 Universities: 2012-2019” and was posted September 18, 2016 and was most recently updated on September 10, 2018 to include the 2019 rankings. It’s only possible to do what we do because of the awesome data! http://andyreiter.com/datasets/ (A base of University and Liberal Arts ranking datasets) https://publicuniversityhonors.com/2016/09/18/average-u-s-news-rankings-for-126-universities-2010-1017/ (Where we have access to missing University rankings that allow us to input manually) https://catalog.data.gov/dataset/college-scorecard (Three big datasets that founds our analysis) https://collegescorecard.ed.gov/data/documentation/ (Big dataset dictionary) 3.2 Our data dictionary Variable Name Description Institution Name (INSTNM) name of college/university Region (REGION) region-wise (e.g. Minnesota is in the Plains region) Setting (LOCALE) setting (large city, small town, etc.) Size of School (UGDS) number of undergraduate degree-seeking students Admission Rate (ADM_RATE) college’s admission rate SAT Scores (SAT_AVG) combined SAT average by year, interchangeable with SAT Racial Diversity (UGDS_WHITE) racial diversity by percent of white students Average Cost Per Year (COSTT4_A) average cost of attendance per year 3.3 Changes to datasets To ensure the consistency among datasets, we made a few necessary changes to the original datasets. For University ranking datasets: We collected and manually added rankings of 60 universities between 2012 and 2019. For Liberal Arts College ranking datasets: To avoid repetition, we changed Westminster College in Pennsylvania to “Westminster College-PA”; changed Wheaton College in Massachusetts to “Wheaton College-MA”. We deleted US Military Academy, US Air Force Academy, US Naval Academy because the infomration is not disclosed to the public and thus not available for viewing. We deleted Grove City College and Principia College because their information were not available in the mega datasets. We made slight changes on a few colleges, such as “St. Norbert College” to “Saint Norbert College” in order to match with the name in mega datasets. For Mega datasets: To avoid repetition, we changed Westminster College in Pennsylvania to “Westminster College-PA”; changed Wheaton College in Massachusetts to “Wheaton College-MA”. We renamed the unranked St. John’s College by its geographic location; renamed the two unranked Union Colleges by their geographic locations. "],
["data-cleaning.html", "Chapter 4 Data cleaning 4.1 What did we do?", " Chapter 4 Data cleaning 4.1 What did we do? If you don’t want to look at all the lengthy work, Kavya and Zuofu are excited to talk through the process with you! library(readxl) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(ggplot2) library(rjags) ## Loading required package: coda ## Linked to JAGS 4.3.0 ## Loaded modules: basemod,bugs library(forcats) library(gridExtra) ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine 4.1.1 Load data, select and rename columns myData1617 &lt;- read.csv(file = &quot;/Users/kevinhuang/Desktop/Spring 2019/MATH 454/454 Project/CollegeScorecard_Raw_Data/MERGED2016_17_PP.csv&quot;) myData1617_sub &lt;- myData1617 %&gt;% select(INSTNM,REGION,LOCALE,UGDS,ADM_RATE,ACTCMMID,SAT_AVG,UGDS_WHITE,COSTT4_A) names(myData1617_sub) &lt;- c(&quot;INSTNM&quot;, &quot;REGION&quot;,&quot;LOCALE&quot;,&quot;UGDS_1617&quot;,&quot;ADM_RATE_1617&quot;,&quot;ACTCMMID_1617&quot;,&quot;SAT_AVG_1617&quot;,&quot;UGDS_WHITE_1617&quot;,&quot;COSTT4_A_1617&quot;) myData1516 &lt;- read.csv(file = &quot;/Users/kevinhuang/Desktop/Spring 2019/MATH 454/454 Project/CollegeScorecard_Raw_Data/MERGED2015_16_PP.csv&quot;) myData1516_sub &lt;- myData1516 %&gt;% select(INSTNM,UGDS,ADM_RATE,ACTCMMID,SAT_AVG,UGDS_WHITE,COSTT4_A) names(myData1516_sub) &lt;- c(&quot;INSTNM&quot;,&quot;UGDS_1516&quot;,&quot;ADM_RATE_1516&quot;,&quot;ACTCMMID_1516&quot;,&quot;SAT_AVG_1516&quot;,&quot;UGDS_WHITE_1516&quot;,&quot;COSTT4_A_1516&quot;) myData1415 &lt;- read.csv(file = &quot;/Users/kevinhuang/Desktop/Spring 2019/MATH 454/454 Project/CollegeScorecard_Raw_Data/MERGED2014_15_PP.csv&quot;) myData1415_sub &lt;- myData1415 %&gt;% select(INSTNM,UGDS,ADM_RATE,ACTCMMID,SAT_AVG,UGDS_WHITE,COSTT4_A) names(myData1415_sub) &lt;- c(&quot;INSTNM&quot;,&quot;UGDS_1415&quot;,&quot;ADM_RATE_1415&quot;,&quot;ACTCMMID_1415&quot;,&quot;SAT_AVG_1415&quot;,&quot;UGDS_WHITE_1415&quot;,&quot;COSTT4_A_1415&quot;) USNewsUniversity &lt;- read_excel(&quot;/Users/kevinhuang/Desktop/Spring 2019/MATH 454/454 Project/USNewsUniversity.xlsx&quot;) newUniversity &lt;- USNewsUniversity %&gt;% select(c(UniversityName, State, Y2019, Y2018, Y2017, Y2016, Y2015, Y2014, Y2013, Y2012)) %&gt;% na.omit() USNewsLiberalArts &lt;- read_excel(&quot;/Users/kevinhuang/Desktop/Spring 2019/MATH 454/454 Project/USNewsLiberalArts.xlsx&quot;) newLiberalArts &lt;- USNewsLiberalArts %&gt;% select(c(CollegeName, State, Y2019, Y2018, Y2017, Y2016, Y2015, Y2014, Y2013, Y2012)) %&gt;% na.omit() 4.1.2 Filter schools that have a rank college &lt;- rep(NA,149) for (i in 1:149){ college[i] = newLiberalArts[i,]$CollegeName } university &lt;- rep(NA,120) for (i in 1:120){ university[i] = newUniversity[i,]$UniversityName } schools &lt;- c(college, university) # Run tests to make sure that datasets are consistent: check_1_Data1617 &lt;- myData1617_sub %&gt;% filter(INSTNM %in% college) dim(check_1_Data1617) check_3_Data1415 &lt;- myData1415_sub %&gt;% filter(INSTNM %in% college) dim(check_3_Data1415) check_2_Data1516 &lt;- myData1516_sub %&gt;% filter(INSTNM %in% college) dim(check_2_Data1516) # Identify class of each variable, in preparation for transformations. class(myData1617_sub$UGDS_1617) class(myData1617_sub$ADM_RATE_1617) class(myData1617_sub$ACTCMMID_1617) class(myData1617_sub$SAT_AVG_1617) class(myData1617_sub$UGDS_WHITE_1617) class(myData1617_sub$COSTT4_A_1617) class(myData1415_sub$INSTNM) myData1415_sub_characterCollege &lt;- myData1415_sub %&gt;% filter(INSTNM %in% college) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1415 = as.numeric(as.character(UGDS_1415))) %&gt;% mutate(UGDS_WHITE_1415 = as.numeric(as.character(UGDS_WHITE_1415))) %&gt;% mutate(SAT_AVG_1415 = as.numeric(as.character(SAT_AVG_1415))) %&gt;% mutate(ACTCMMID_1415 = as.numeric(as.character(ACTCMMID_1415))) %&gt;% mutate(ADM_RATE_1415 = as.numeric(as.character(ADM_RATE_1415))) %&gt;% mutate(COSTT4_A_1415 = as.numeric(as.character(COSTT4_A_1415))) ## Warning in evalq(as.numeric(as.character(SAT_AVG_1415)), &lt;environment&gt;): 强 ## 制改变过程中产生了NA ## Warning in evalq(as.numeric(as.character(ACTCMMID_1415)), &lt;environment&gt;): ## 强制改变过程中产生了NA ## Warning in evalq(as.numeric(as.character(ADM_RATE_1415)), &lt;environment&gt;): ## 强制改变过程中产生了NA myData1516_sub_characterCollege &lt;- myData1516_sub %&gt;% filter(INSTNM %in% college) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1516 = as.numeric(as.character(UGDS_1516))) %&gt;% mutate(UGDS_WHITE_1516 = as.numeric(as.character(UGDS_WHITE_1516))) %&gt;% mutate(SAT_AVG_1516 = as.numeric(as.character(SAT_AVG_1516))) %&gt;% mutate(ACTCMMID_1516 = as.numeric(as.character(ACTCMMID_1516))) %&gt;% mutate(ADM_RATE_1516 = as.numeric(as.character(ADM_RATE_1516))) %&gt;% mutate(COSTT4_A_1516 = as.numeric(as.character(COSTT4_A_1516))) ## Warning in evalq(as.numeric(as.character(SAT_AVG_1516)), &lt;environment&gt;): 强 ## 制改变过程中产生了NA ## Warning in evalq(as.numeric(as.character(ACTCMMID_1516)), &lt;environment&gt;): ## 强制改变过程中产生了NA myData1617_sub_characterCollege &lt;- myData1617_sub %&gt;% filter(INSTNM %in% college) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1617 = as.numeric(as.character(UGDS_1617))) %&gt;% mutate(UGDS_WHITE_1617 = as.numeric(as.character(UGDS_WHITE_1617))) %&gt;% mutate(SAT_AVG_1617 = as.numeric(as.character(SAT_AVG_1617))) %&gt;% mutate(ACTCMMID_1617 = as.numeric(as.character(ACTCMMID_1617))) %&gt;% mutate(ADM_RATE_1617 = as.numeric(as.character(ADM_RATE_1617))) %&gt;% mutate(COSTT4_A_1617 = as.numeric(as.character(COSTT4_A_1617))) ## Warning in evalq(as.numeric(as.character(SAT_AVG_1617)), &lt;environment&gt;): 强 ## 制改变过程中产生了NA ## Warning in evalq(as.numeric(as.character(ACTCMMID_1617)), &lt;environment&gt;): ## 强制改变过程中产生了NA # To recheck that all three datasets are the same with regard to college names. dim(myData1415_sub_characterCollege) dim(myData1516_sub_characterCollege) dim(myData1617_sub_characterCollege) 4.1.3 Join datasets fullLiberalArts &lt;- newLiberalArts %&gt;% full_join(myData1415_sub_characterCollege, by = c(&quot;CollegeName&quot; = &quot;INSTNM&quot;)) %&gt;% full_join(myData1516_sub_characterCollege, by = c(&quot;CollegeName&quot; = &quot;INSTNM&quot;)) %&gt;% full_join(myData1617_sub_characterCollege, by = c(&quot;CollegeName&quot; = &quot;INSTNM&quot;)) dim(fullLiberalArts) ## [1] 175 30 full_LiberalArts &lt;- fullLiberalArts[!duplicated(fullLiberalArts$CollegeName), ] dim(full_LiberalArts) # After we removed the repetitive entries. ## [1] 149 30 myData1415_sub_characterUniversity &lt;- myData1415_sub %&gt;% filter(INSTNM %in% university) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1415 = as.numeric(as.character(UGDS_1415))) %&gt;% mutate(UGDS_WHITE_1415 = as.numeric(as.character(UGDS_WHITE_1415))) %&gt;% mutate(SAT_AVG_1415 = as.numeric(as.character(SAT_AVG_1415))) %&gt;% mutate(ACTCMMID_1415 = as.numeric(as.character(ACTCMMID_1415))) %&gt;% mutate(ADM_RATE_1415 = as.numeric(as.character(ADM_RATE_1415))) %&gt;% mutate(COSTT4_A_1415 = as.numeric(as.character(COSTT4_A_1415))) ## Warning in evalq(as.numeric(as.character(SAT_AVG_1415)), &lt;environment&gt;): 强 ## 制改变过程中产生了NA ## Warning in evalq(as.numeric(as.character(ACTCMMID_1415)), &lt;environment&gt;): ## 强制改变过程中产生了NA myData1516_sub_characterUniversity &lt;- myData1516_sub %&gt;% filter(INSTNM %in% university) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1516 = as.numeric(as.character(UGDS_1516))) %&gt;% mutate(UGDS_WHITE_1516 = as.numeric(as.character(UGDS_WHITE_1516))) %&gt;% mutate(SAT_AVG_1516 = as.numeric(as.character(SAT_AVG_1516))) %&gt;% mutate(ACTCMMID_1516 = as.numeric(as.character(ACTCMMID_1516))) %&gt;% mutate(ADM_RATE_1516 = as.numeric(as.character(ADM_RATE_1516))) %&gt;% mutate(COSTT4_A_1516 = as.numeric(as.character(COSTT4_A_1516))) ## Warning in evalq(as.numeric(as.character(SAT_AVG_1516)), &lt;environment&gt;): 强 ## 制改变过程中产生了NA ## Warning in evalq(as.numeric(as.character(ACTCMMID_1516)), &lt;environment&gt;): ## 强制改变过程中产生了NA myData1617_sub_characterUniversity &lt;- myData1617_sub %&gt;% filter(INSTNM %in% university) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1617 = as.numeric(as.character(UGDS_1617))) %&gt;% mutate(UGDS_WHITE_1617 = as.numeric(as.character(UGDS_WHITE_1617))) %&gt;% mutate(SAT_AVG_1617 = as.numeric(as.character(SAT_AVG_1617))) %&gt;% mutate(ACTCMMID_1617 = as.numeric(as.character(ACTCMMID_1617))) %&gt;% mutate(ADM_RATE_1617 = as.numeric(as.character(ADM_RATE_1617))) %&gt;% mutate(COSTT4_A_1617 = as.numeric(as.character(COSTT4_A_1617))) ## Warning in evalq(as.numeric(as.character(SAT_AVG_1617)), &lt;environment&gt;): 强 ## 制改变过程中产生了NA ## Warning in evalq(as.numeric(as.character(ACTCMMID_1617)), &lt;environment&gt;): ## 强制改变过程中产生了NA fullUniversity &lt;- newUniversity %&gt;% full_join(myData1415_sub_characterUniversity, by = c(&quot;UniversityName&quot; = &quot;INSTNM&quot;)) %&gt;% full_join(myData1516_sub_characterUniversity, by = c(&quot;UniversityName&quot; = &quot;INSTNM&quot;)) %&gt;% full_join(myData1617_sub_characterUniversity, by = c(&quot;UniversityName&quot; = &quot;INSTNM&quot;)) dim(fullUniversity) ## [1] 120 30 "],
["visualization.html", "Chapter 5 Visualization 5.1 1st &amp; 2nd Peeks 5.2 3rd Peek 5.3 4th Peek", " Chapter 5 Visualization Intuitively, what’s going on? 5.1 1st &amp; 2nd Peeks In our first and second peeks, we are interested in geographical locations of universities and colleges. It is surprising to learn that students, when choosing schools, give much weight to a school’s geographical location. We investigate two variables that contribute to prospective students’ choice, giving side-by-side plots that reflect how the variables vary between universities and liberal arts colleges. REGION_collapse &lt;- fct_collapse(as.character(full_LiberalArts$REGION), NewEngland = &quot;1&quot;, MidEast = &quot;2&quot;, GreatLakes = &quot;3&quot;, Plains = &quot;4&quot;, Southeast = &quot;5&quot;, Southwest = &quot;6&quot;, RockyMountains = &quot;7&quot;, FarWest = &quot;8&quot;) full_LiberalArts$REGION_collapse &lt;- REGION_collapse LOCALE_collapse_lac1 &lt;- fct_collapse(full_LiberalArts$LOCALE, City = c(&quot;11&quot;,&quot;12&quot;,&quot;13&quot;), Suburb = c(&quot;21&quot;,&quot;22&quot;,&quot;23&quot;), Town = c(&quot;31&quot;,&quot;32&quot;,&quot;33&quot;), Rural = c(&quot;41&quot;,&quot;42&quot;,&quot;43&quot;)) LOCALE_collapse_lac2 &lt;- fct_collapse(full_LiberalArts$LOCALE, LargeCity = &quot;11&quot;, MidsizeCity = &quot;12&quot;, SmallCity = &quot;13&quot;, LargeSuburb = &quot;21&quot;, MidsizeSuburb = &quot;22&quot;, SmallSuburb = &quot;23&quot;, FringeTown = &quot;31&quot;, DistantTown = &quot;32&quot;, RemoteTown = &quot;33&quot;, FringeRural = &quot;41&quot;, DistantRural = &quot;42&quot;, RemoteTown = &quot;43&quot;) full_LiberalArts$LOCALE_collapse_lac1 &lt;- LOCALE_collapse_lac1 full_LiberalArts$LOCALE_collapse_lac2 &lt;- LOCALE_collapse_lac2 REGION_collapse &lt;- fct_collapse(as.character(fullUniversity$REGION), NewEngland = &quot;1&quot;, MidEast = &quot;2&quot;, GreatLakes = &quot;3&quot;, Plains = &quot;4&quot;, Southeast = &quot;5&quot;, Southwest = &quot;6&quot;, RockyMountains = &quot;7&quot;, FarWest = &quot;8&quot;) fullUniversity$REGION_collapse &lt;- REGION_collapse LOCALE_collapse &lt;- fct_collapse(fullUniversity$LOCALE, City = c(&quot;11&quot;,&quot;12&quot;,&quot;13&quot;), Suburb = c(&quot;21&quot;,&quot;22&quot;,&quot;23&quot;), Town = c(&quot;31&quot;,&quot;32&quot;,&quot;33&quot;), Rural = c(&quot;41&quot;,&quot;42&quot;,&quot;43&quot;)) LOCALE_collapse2 &lt;- fct_collapse(fullUniversity$LOCALE, LargeCity = &quot;11&quot;, MidsizeCity = &quot;12&quot;, SmallCity = &quot;13&quot;, LargeSuburb = &quot;21&quot;, MidsizeSuburb = &quot;22&quot;, SmallSuburb = &quot;23&quot;, FringeTown = &quot;31&quot;, DistantTown = &quot;32&quot;, RemoteTown = &quot;33&quot;, FringeRural = &quot;41&quot;, DistantRural = &quot;42&quot;, RemoteTown = &quot;43&quot;) fullUniversity$LOCALE_collapse &lt;- LOCALE_collapse fullUniversity$LOCALE_collapse2 &lt;- LOCALE_collapse2 g1 &lt;- ggplot(fullUniversity, aes(x = REGION_collapse, fill = LOCALE_collapse)) + geom_bar() + xlab(&quot;Region&quot;) + scale_fill_discrete(name = &quot;Breakdown&quot;) + ylab(&quot;Count&quot;) + ggtitle(&quot;Region of Universities, broken down by location&quot;) + theme(axis.text.x = element_text(angle = 30, hjust = 1)) g2 &lt;- ggplot(full_LiberalArts, aes(x = REGION_collapse, fill = LOCALE_collapse_lac1)) + geom_bar() + xlab(&quot;Region&quot;) + scale_fill_discrete(name = &quot;Breakdown&quot;) + ylab(&quot;Count&quot;) + ggtitle(&quot;Region of Liberal Arts Colleges, broken down by location&quot;) + theme(axis.text.x = element_text(angle = 30, hjust = 1)) grid.arrange(g1, g2, nrow = 2) g3 &lt;- ggplot(fullUniversity, aes(x = LOCALE_collapse, fill = LOCALE_collapse2)) + geom_bar() + xlab(&quot;Geographical Location&quot;) + scale_fill_discrete(name = &quot;Breakdown&quot;) + ylab(&quot;Count&quot;) + ggtitle(&quot;Location of Universities, broken down by size&quot;) + theme(plot.title = element_text(size = 10)) g4 &lt;- ggplot(full_LiberalArts, aes(x = LOCALE_collapse_lac1, fill = LOCALE_collapse_lac2)) + geom_bar() + xlab(&quot;Geographical Location&quot;) + scale_fill_discrete(name = &quot;Breakdown&quot;) + ylab(&quot;Count&quot;) + ggtitle(&quot;Location of Liberal Arts Colleges, broken down by size&quot;) + theme(plot.title = element_text(size = 9.5)) + theme(axis.text.x = element_text(angle = 30)) grid.arrange(g3, g4, nrow = 1) 5.2 3rd Peek “In a utilitarian age, of all other times, it is a matter of grave importance that fairy tales should be respected.” -Charles Dickens Here’s when people tend to (have the excuse to) get a little more utilitarian. Studies find that people pay more attention to ranking when ranking data is available. We (or more precisely, Zuofu) are just like that. SAT is usually one of the major indicators of students’ fitness with certain colleges, both from students’ and colleges’ perspective. We do a bi-variate visualization between average SAT score of 2016-2017 academic year and university ranking of Year 2018. If you look further in our project (which hopefully you do!), you will see this is the basis of our first introductory model. p1 &lt;- ggplot(fullUniversity, aes(x = SAT_AVG_1617, y = as.numeric(Y2018))) + geom_point() + geom_smooth(method = &quot;lm&quot;) + xlim(1000,1600) + ylim(1,150) + xlab(&quot;Average SAT Score&quot;) + ylab(&quot;School Ranking 2018&quot;) + ggtitle(&quot;Universities&quot;) p2 &lt;- ggplot(full_LiberalArts, aes(x = SAT_AVG_1617, y = as.numeric(Y2018))) + geom_point() + geom_smooth(method = &quot;lm&quot;) + xlim(1000,1600) + ylim(1,150) + xlab(&quot;Average SAT Score&quot;) + ylab(&quot;School Ranking 2018&quot;) + ggtitle(&quot;Liberal Arts Colleges&quot;) grid.arrange(p1, p2, nrow = 1) ## Warning: Removed 7 rows containing non-finite values (stat_smooth). ## Warning: Removed 7 rows containing missing values (geom_point). ## Warning: Removed 8 rows containing missing values (geom_smooth). ## Warning in FUN(X[[i]], ...): 强制改变过程中产生了NA ## Warning in FUN(X[[i]], ...): 强制改变过程中产生了NA ## Warning: Removed 54 rows containing non-finite values (stat_smooth). ## Warning: Removed 54 rows containing missing values (geom_point). ## Warning: Removed 9 rows containing missing values (geom_smooth). 5.3 4th Peek “Diversity: the art of thinking independently together.” -Malcolm Forbes In an effort to create a more just and inclusive world, the opportunity to engage with people of different backgrounds and identities plays an increasingly important role. We plan to look into the cmommon trend between the size of the student body and diversity factors (namely, the precentage of white students and the cost of attending college). fullUniversity$UGDS_WHITE_1617_categ = cut(fullUniversity$UGDS_WHITE_1617, c(0,0.2,0.4,0.6,0.8,1)) ggplot(fullUniversity, aes(x = UGDS_1617, y = UGDS_WHITE_1617, color = COSTT4_A_1617)) + geom_point() + xlab(&quot;Size of Undergraduate Student Body&quot;) + scale_fill_discrete(name = &quot;Cost for Year 2017&quot;) + ylab(&quot;Percentage of White Students&quot;) + ggtitle(&quot;Economic and Racial Diversity of Universities&quot;) We didn’t see an observable relationship between cost and the percentage of white students; we did observe that universities of a smaller size tend to correlate with a much larger cost. full_LiberalArts$UGDS_WHITE_1617_categ = cut(full_LiberalArts$UGDS_WHITE_1617, c(0,0.2,0.4,0.6,0.8,1)) ggplot(full_LiberalArts, aes(x = UGDS_1617, y = UGDS_WHITE_1617, color = COSTT4_A_1617)) + geom_point() + xlab(&quot;Size of Undergraduate Student Body&quot;) + scale_fill_discrete(name = &quot;Cost for Year 2017&quot;) + ylab(&quot;Percentage of White Students&quot;) + ggtitle(&quot;Economic and Racial Diversity of Liberal Arts Colleges&quot;) ## Warning: Removed 2 rows containing missing values (geom_point). In terms of liberal arts colleges, we can see that the data points (schools) are more clustered at the top section of the plot. This makes sense to us given that most liberal arts colleges are private compared to universities. We didn’t see an observable relationship between cost and the percentage of white students "],
["bayesian-models-part-1-1-university.html", "Chapter 6 Bayesian Models Part 1.1 University 6.1 Model 0 6.2 Model 1 6.3 Model 2 6.4 Future steps", " Chapter 6 Bayesian Models Part 1.1 University How will we simulate what we want to know? HINT: Start SIMPLE 6.1 Model 0 Ranking by one year of SAT score: with intuition from \\(\\text{hist}(\\sqrt{1/rgamma(10000,a,b)})\\). 6.1.1 First Impression ggplot(fullUniversity, aes(x = SAT_AVG_1617, y = as.numeric(Y2018))) + geom_point() + geom_smooth(method = &quot;lm&quot;) + xlim(1000,1600) + ylim(1,150) + xlab(&quot;Average SAT Score&quot;) + ylab(&quot;School Ranking 2018&quot;) ## Warning: Removed 7 rows containing non-finite values (stat_smooth). ## Warning: Removed 7 rows containing missing values (geom_point). ## Warning: Removed 8 rows containing missing values (geom_smooth). It appears to be a linear relationship! 6.1.2 Building the model # DEFINE the model university_model_0 &lt;- &quot;model{ for(i in 1:length(y)) { # Data model y[i] ~ dnorm(beta0 + beta1 * x[i], tau0) } # Priors for theta beta0 ~ dnorm(300,1/250000) beta1 ~ dnorm(0, 1/100) tau0 ~ dgamma(7,4000) }&quot; # COMPILE the model model_data0 &lt;- data.frame(y = fullUniversity$Y2018, x = fullUniversity$SAT_AVG_1617) model_data0 &lt;- na.omit(model_data0) university_jags_0 &lt;- jags.model(textConnection(university_model_0), data = list(y = model_data0$y, x = model_data0$x), inits = list(.RNG.name = &quot;base::Wichmann-Hill&quot;, .RNG.seed = 454)) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 114 ## Unobserved stochastic nodes: 3 ## Total graph size: 440 ## ## Initializing model # SIMULATE the model university_sim_0 &lt;- coda.samples(university_jags_0, variable.names = c(&quot;beta0&quot;, &quot;beta1&quot;, &quot;tau0&quot;), n.iter = 10000) # STORE the chains in a data frame university_chains_0 &lt;- data.frame(university_sim_0[[1]]) 6.1.3 Model summary summary(university_sim_0) ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## beta0 403.411563 3.227e+01 3.227e-01 4.714e+00 ## beta1 -0.263423 2.472e-02 2.472e-04 3.617e-03 ## tau0 0.002421 3.279e-04 3.279e-06 1.303e-05 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## beta0 355.59133 390.793008 405.743679 421.198553 447.191343 ## beta1 -0.29699 -0.277127 -0.265219 -0.253713 -0.226779 ## tau0 0.00182 0.002212 0.002415 0.002628 0.003062 6.1.4 Posterior inference For an unknown university with a mean student SAT score of 1450 (e.g. Bvictor University), we could predict its ranking from our rjags simulation. university_chains_0 &lt;- university_chains_0 %&gt;% mutate(ranking_new = rnorm(10000, mean = beta0 + beta1*1450, sd = (1/tau0)^(1/2))) university_chains_0 %&gt;% summarize(quantile(ranking_new,0.025),quantile(ranking_new,0.975)) ## quantile(ranking_new, 0.025) quantile(ranking_new, 0.975) ## 1 -20.52397 63.32838 A \\(95\\%\\) credible interval is \\((-20,62)\\). Due to restrictions of this one-predictor model, we expect to see an improvement in later models. 6.2 Model 1 Ranking by three years of SAT score. By analysis, the three predictors are multicollinear, meaning that this may not be a significantly better model than Model 0. 6.2.1 First Impression It’s harder to make predictions (and as it turns out, hard to make visualizations as well) with three years’ SAT score in the same linear model, as we can foresee significant multicollinearity. This is clearly seen that the coefficient for SAT average of Year 2014-15 (SAT_AVG_1415) variable is positive, meaning that school ranking worsens as the average SAT score increases. To further test our thinking, we created a linear model university_linear_1. From the ANOVA summary, we see that any one of these variables should be sufficient. university_linear_1 &lt;- lm(as.numeric(Y2018) ~ SAT_AVG_1617 + SAT_AVG_1516 + SAT_AVG_1415, fullUniversity) summary(university_linear_1) ## ## Call: ## lm(formula = as.numeric(Y2018) ~ SAT_AVG_1617 + SAT_AVG_1516 + ## SAT_AVG_1415, data = fullUniversity) ## ## Residuals: ## Min 1Q Median 3Q Max ## -104.127 -8.988 -2.062 12.129 38.789 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 415.49873 20.70087 20.072 &lt;2e-16 *** ## SAT_AVG_1617 -0.04769 0.14485 -0.329 0.743 ## SAT_AVG_1516 -0.26890 0.18977 -1.417 0.159 ## SAT_AVG_1415 0.04301 0.16163 0.266 0.791 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.68 on 110 degrees of freedom ## (6 observations deleted due to missingness) ## Multiple R-squared: 0.7292, Adjusted R-squared: 0.7218 ## F-statistic: 98.75 on 3 and 110 DF, p-value: &lt; 2.2e-16 anova(university_linear_1) ## Analysis of Variance Table ## ## Response: as.numeric(Y2018) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## SAT_AVG_1617 1 113627 113627 293.4484 &lt;2e-16 *** ## SAT_AVG_1516 1 1052 1052 2.7173 0.1021 ## SAT_AVG_1415 1 27 27 0.0708 0.7907 ## Residuals 110 42594 387 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For the sake of scientific experiment (and to catch the audience’s interest), we still did a Bayesian model and Bayesian posterior inference. We found out that this model does a slightly better job at predicting college rankings (by eliminating negative rankings) because it naturally contains more information as we increase the number of predictors. The change is not significant enough to call it an improvement. 6.2.2 Building the model university_model_1 &lt;- &quot;model{ # Data: observations for(i in 1:length(y)) { y[i] ~ dnorm(beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x3[i], tau_big) } # Data: subjects beta0 ~ dnorm(0,1/250000) beta1 ~ dnorm(0,100) beta2 ~ dnorm(0,100) beta3 ~ dnorm(0,100) tau_big ~ dgamma(7,1000) }&quot; # COMPILE y &lt;- fullUniversity$Y2018 model_data1 &lt;- as.data.frame(cbind(y, x1 = fullUniversity$SAT_AVG_1415, x2 = fullUniversity$SAT_AVG_1516, x3 = fullUniversity$SAT_AVG_1617)) model_data1 &lt;- na.omit(model_data1) university_jags_1 &lt;- jags.model(textConnection(university_model_1), data = list(y = model_data1$y, x1 = model_data1$x1, x2 = model_data1$x2, x3 = model_data1$x3), inits=list(.RNG.name = &quot;base::Wichmann-Hill&quot;, .RNG.seed = 454)) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 114 ## Unobserved stochastic nodes: 5 ## Total graph size: 876 ## ## Initializing model # SIMULATE the model university_sim_1 &lt;- coda.samples(university_jags_1, variable.names = c(&quot;beta0&quot;,&quot;beta1&quot;,&quot;beta2&quot;,&quot;beta3&quot;,&quot;tau_big&quot;), n.iter = 10000) # STORE the chains in a data frame university_chains_1 &lt;- data.frame(university_sim_1[[1]]) 6.2.3 Model summary summary(university_sim_1) ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## beta0 409.864989 3.553e+01 3.553e-01 5.783e+00 ## beta1 0.001373 1.078e-01 1.078e-03 1.120e-01 ## beta2 -0.179656 7.938e-02 7.938e-04 4.684e-02 ## beta3 -0.090932 7.417e-02 7.417e-04 5.086e-02 ## tau_big 0.002781 4.008e-04 4.008e-06 2.714e-05 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## beta0 345.977738 400.686962 414.807439 427.055384 447.427911 ## beta1 -0.224077 -0.074142 0.033869 0.084271 0.127709 ## beta2 -0.304552 -0.247353 -0.181364 -0.127231 -0.006660 ## beta3 -0.212426 -0.151206 -0.106569 -0.025275 0.034454 ## tau_big 0.002051 0.002538 0.002778 0.003029 0.003556 6.2.4 Posterior inference For an unknown university with three years’ mean student SAT score of 1450, 1440, 1420 (e.g. Cvictor University), we could predict its ranking from our rjags simulation. university_chains_1 &lt;- university_chains_1 %&gt;% mutate(ranking_new = rnorm(10000, mean = beta0 + beta1*1450 + beta2*1440 +beta3*1420, sd = (1/tau_big)^(1/2))) university_chains_1 %&gt;% summarize(quantile(ranking_new,0.025),quantile(ranking_new,0.975)) ## quantile(ranking_new, 0.025) quantile(ranking_new, 0.975) ## 1 -14.09145 65.126 A \\(95\\%\\) credible interval is \\((-14,64)\\). The \\(95\\%\\) credible interval in the new model does a better job at eliminating negative rankings, which are impossible in reality. 6.3 Model 2 2018 U.S. News Ranking by average SAT score and admissions rate of Year 2017. 6.3.1 First Impression ggplot(fullUniversity, aes(x = ADM_RATE_1617, y = as.numeric(Y2018), color = SAT_AVG_1617)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + xlab(&quot;Admission rate of Year 2016-2017&quot;) + ylab(&quot;School Ranking 2018&quot;) + ggtitle(&quot;University ranking by SAT and admission rate&quot;) There’s a strong positive relationship between the admission rate of colleges and college ranking (a larger number in college ranking is worse). From color gradient of points, we can see that schools with higher SAT averages tend to be better ranked. 6.3.2 Building the model 6.3.2.1 Step 1 A linear model provides some intuition about how the priors might be constructed. summary(lm(fullUniversity$Y2018 ~ fullUniversity$SAT_AVG_1617 + fullUniversity$ADM_RATE_1617)) ## ## Call: ## lm(formula = fullUniversity$Y2018 ~ fullUniversity$SAT_AVG_1617 + ## fullUniversity$ADM_RATE_1617) ## ## Residuals: ## Min 1Q Median 3Q Max ## -78.978 -9.142 -1.986 8.865 43.380 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 172.5262 45.1297 3.823 0.000218 *** ## fullUniversity$SAT_AVG_1617 -0.1150 0.0301 -3.822 0.000219 *** ## fullUniversity$ADM_RATE_1617 84.8854 14.8094 5.732 8.66e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.42 on 111 degrees of freedom ## (6 observations deleted due to missingness) ## Multiple R-squared: 0.7858, Adjusted R-squared: 0.7819 ## F-statistic: 203.6 on 2 and 111 DF, p-value: &lt; 2.2e-16 6.3.2.2 Step 2 university_model_2 &lt;- &quot;model{ # Data: observations for(i in 1:length(y)) { y[i] ~ dnorm(beta0[i] + beta1[i]*x1[i] + beta2[i]*x2[i], tau_big[i]) # Data: subjects beta0[i] ~ dnorm(b0, tau0) beta1[i] ~ dnorm(b1,tau1) beta2[i] ~ dnorm(b2,tau2) tau_big[i] ~ dgamma(s,r) } # Hyperpriors b0 ~ dnorm(180,1/4000) tau0 ~ dnorm(30, 1/9) b1 ~ dnorm(-0.1,1000) tau1 ~ dnorm(1000,1000) b2 ~ dnorm(80,1/100) tau2 ~ dnorm(10,1) s ~ dnorm(7,1) r ~ dnorm(10000, 1/10000) }&quot; # COMPILE y &lt;- fullUniversity$Y2018 model_data2 &lt;- as.data.frame(cbind(y, x1 = fullUniversity$SAT_AVG_1617, x2 = fullUniversity$ADM_RATE_1617)) model_data2 &lt;- na.omit(model_data2) university_jags_2 &lt;- jags.model(textConnection(university_model_2), data = list(y = model_data2$y, x1 = model_data2$x1, x2 = model_data2$x2), inits=list(.RNG.name = &quot;base::Wichmann-Hill&quot;, .RNG.seed = 454)) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 114 ## Unobserved stochastic nodes: 464 ## Total graph size: 1165 ## ## Initializing model # SIMULATE the model university_sim_2 &lt;- coda.samples(university_jags_2, variable.names = c(&quot;b0&quot;,&quot;tau0&quot;,&quot;b1&quot;,&quot;tau1&quot;,&quot;b2&quot;,&quot;tau2&quot;,&quot;s&quot;,&quot;r&quot;), n.iter = 10000) # STORE the chains in a data frame university_chains_2 &lt;- data.frame(university_sim_2[[1]]) 6.3.3 Model summary summary(university_sim_2) ## ## Iterations = 1001:11000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## b0 181.003 0.866642 8.666e-03 0.4845152 ## b1 -0.122 0.003986 3.986e-05 0.0001962 ## b2 86.978 2.748797 2.749e-02 2.0167560 ## r 9978.437 98.495695 9.850e-01 1.3555384 ## s 9.367 0.854744 8.547e-03 0.0428560 ## tau0 29.903 3.051727 3.052e-02 0.0544687 ## tau1 1000.000 0.031393 3.139e-04 0.0003934 ## tau2 10.021 1.000127 1.000e-02 0.0179231 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## b0 179.2258 180.2942 181.163 181.6543 182.5227 ## b1 -0.1298 -0.1247 -0.122 -0.1193 -0.1143 ## b2 80.6675 85.0648 87.714 89.2203 90.5414 ## r 9784.6197 9912.1376 9978.523 10044.9089 10170.4126 ## s 7.7114 8.7880 9.345 9.9331 11.0906 ## tau0 23.8816 27.8340 29.879 31.9392 35.9038 ## tau1 999.9366 999.9792 1000.000 1000.0208 1000.0615 ## tau2 8.0599 9.3477 10.030 10.6819 12.0006 6.3.4 Posterior inference For an unknown university with student mean SAT score of 1450 and an admission rate of \\(30\\%\\) (e.g. Dvictor University), we could predict its ranking from our rjags simulation. university_chains_2 &lt;- university_chains_2 %&gt;% mutate(beta0_new = rnorm(10000,b0,(1/tau0)^(1/2))) %&gt;% mutate(beta1_new = rnorm(10000,b1,(1/tau1)^(1/2))) %&gt;% mutate(beta2_new = rnorm(10000,b2,(1/tau2)^(1/2))) %&gt;% mutate(tau_big_new = rgamma(10000,s,r)) %&gt;% mutate(ranking_new = rnorm(10000, mean = beta0_new + beta1_new * 1450 + beta2_new * 0.3, sd = (1/tau_big_new)^(1/2))) university_chains_2 %&gt;% summarize(quantile(ranking_new,0.025),quantile(ranking_new,0.975)) ## quantile(ranking_new, 0.025) quantile(ranking_new, 0.975) ## 1 -81.76592 142.1052 A \\(95\\%\\) credible interval is \\((-86,142)\\). The \\(95\\%\\) credible interval does a poor job at eliminating negative rankings or making an accurate prediction. We propose that this may be due to the variability of the priors and hyperpriors set in our Hierarchical Model. 6.4 Future steps Next, we plan to incorporate more predictive variables of ranking \\(y_i\\). Since some predictors we choose are correlated, we will also emphasize on reflecting that in our models. "],
["bayesian-models-part-1-2-liberal-arts-colleges.html", "Chapter 7 Bayesian Models Part 1.2 Liberal Arts Colleges 7.1 Model 0 7.2 Model 1 7.3 Model 2 7.4 Future steps", " Chapter 7 Bayesian Models Part 1.2 Liberal Arts Colleges Hint AGAIN: Start simple The real question: why do we repeat the process on liberal arts colleges again? Answer: Liberal arts colleges fall under an entirely separate ranking list and often have different characteristics (e.g. size and location, private vs public). We repeat the process to observe the similarity and differences between universities and liberal arts colleges. 7.1 Model 0 Ranking by one year of SAT score: with intuition from \\(\\text{hist}(\\sqrt{1/rgamma(10000,a,b)})\\). 7.1.1 First Impression ggplot(full_LiberalArts, aes(x = SAT_AVG_1617, y = as.numeric(Y2018))) + geom_point() + geom_smooth(method = &quot;lm&quot;) + xlim(1000,1600) + ylim(1,150) + xlab(&quot;Average SAT Score&quot;) + ylab(&quot;School Ranking 2018&quot;) ## Warning in FUN(X[[i]], ...): 强制改变过程中产生了NA ## Warning in FUN(X[[i]], ...): 强制改变过程中产生了NA ## Warning: Removed 54 rows containing non-finite values (stat_smooth). ## Warning: Removed 54 rows containing missing values (geom_point). ## Warning: Removed 9 rows containing missing values (geom_smooth). It appears to be a linear relationship! 7.1.2 Building the model # DEFINE the model liberal_model_0 &lt;- &quot;model{ for(i in 1:length(y)) { # Data model y[i] ~ dnorm(beta0 + beta1 * x[i], tau0) } # Priors for theta beta0 ~ dnorm(300,1/250000) beta1 ~ dnorm(0, 1/100) tau0 ~ dgamma(7,4000) }&quot; # COMPILE the model model_data4 &lt;- data.frame(y = full_LiberalArts$Y2018, x = full_LiberalArts$SAT_AVG_1617) model_data4 &lt;- na.omit(model_data4) liberal_jags_0 &lt;- jags.model(textConnection(liberal_model_0), data = list(y = model_data4$y, x = model_data4$x), inits = list(.RNG.name = &quot;base::Wichmann-Hill&quot;, .RNG.seed = 454)) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 101 ## Unobserved stochastic nodes: 3 ## Total graph size: 396 ## ## Initializing model # SIMULATE the model liberal_sim_0 &lt;- coda.samples(liberal_jags_0, variable.names = c(&quot;beta0&quot;, &quot;beta1&quot;, &quot;tau0&quot;), n.iter = 10000) # STORE the chains in a data frame liberal_chains_0 &lt;- data.frame(liberal_sim_0[[1]]) 7.1.3 Model summary summary(liberal_sim_0) ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## beta0 -0.614922 1.583e+01 1.583e-01 2.144e+00 ## beta1 0.021724 1.296e-02 1.296e-04 1.767e-03 ## tau0 0.003849 5.113e-04 5.113e-06 5.914e-06 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## beta0 -31.655750 -11.608615 -1.094746 10.751901 29.460108 ## beta1 -0.003149 0.012368 0.022064 0.030707 0.047298 ## tau0 0.002903 0.003495 0.003829 0.004177 0.004907 7.1.4 Posterior inference For an unknown liberal arts college with a mean student SAT score of 1450 (e.g. Bvictor College), we could predict its ranking from our rjags simulation. liberal_chains_0 &lt;- liberal_chains_0 %&gt;% mutate(ranking_new = rnorm(10000, mean = beta0 + beta1*1450, sd = (1/tau0)^(1/2))) liberal_chains_0 %&gt;% summarize(quantile(ranking_new,0.025),quantile(ranking_new,0.975)) ## quantile(ranking_new, 0.025) quantile(ranking_new, 0.975) ## 1 -1.58344 64.12103 A \\(95\\%\\) credible interval is \\((-1,63)\\). Due to restrictions of this one-predictor model, we expect to see an improvement in later models. 7.2 Model 1 Ranking by three years of SAT score. This is the best Bayesian model one can ever come up with. Just Kidding. As we discussed earlier in Chapter 6, Model 1 is overwhelmingly multicollinear; Model 1 is a sufficient sustitute if we want to look at the relationship between ranking and SAT score. 7.3 Model 2 2018 U.S. News Ranking by average SAT score and admissions rate of Year 2017. 7.3.1 First Impression ggplot(full_LiberalArts, aes(x = ADM_RATE_1617, y = as.numeric(Y2018), color = SAT_AVG_1617)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + xlab(&quot;Admission rate of Year 2016-2017&quot;) + ylab(&quot;School Ranking 2018&quot;) + ggtitle(&quot;Liberal Arts College ranking by SAT and admission rate&quot;) ## Warning in FUN(X[[i]], ...): 强制改变过程中产生了NA ## Warning in FUN(X[[i]], ...): 强制改变过程中产生了NA ## Warning in FUN(X[[i]], ...): 强制改变过程中产生了NA ## Warning: Removed 2 rows containing non-finite values (stat_smooth). ## Warning: Removed 2 rows containing missing values (geom_point). There’s a strong positive relationship between the admission rate of colleges and college ranking (a larger number in college ranking is worse). From color gradient of points, we can see that schools with higher SAT averages tend to be better ranked. 7.3.2 Building the model 7.3.2.1 Step 1 A linear model provides some intuition about how the priors might be constructed. summary(lm(full_LiberalArts$Y2018 ~ full_LiberalArts$SAT_AVG_1617 + full_LiberalArts$ADM_RATE_1617)) ## ## Call: ## lm(formula = full_LiberalArts$Y2018 ~ full_LiberalArts$SAT_AVG_1617 + ## full_LiberalArts$ADM_RATE_1617) ## ## Residuals: ## Min 1Q Median 3Q Max ## -50.679 -9.613 -0.042 10.169 59.538 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 430.23894 34.40379 12.506 &lt;2e-16 *** ## full_LiberalArts$SAT_AVG_1617 -0.30402 0.02336 -13.012 &lt;2e-16 *** ## full_LiberalArts$ADM_RATE_1617 37.57722 13.59032 2.765 0.0068 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.48 on 98 degrees of freedom ## (48 observations deleted due to missingness) ## Multiple R-squared: 0.8577, Adjusted R-squared: 0.8548 ## F-statistic: 295.3 on 2 and 98 DF, p-value: &lt; 2.2e-16 7.3.2.2 Step 2 liberal_model_2 &lt;- &quot;model{ # Data: observations for(i in 1:length(y)) { y[i] ~ dnorm(beta0[i] + beta1[i]*x1[i] + beta2[i]*x2[i], tau_big[i]) # Data: subjects beta0[i] ~ dnorm(b0, tau0) beta1[i] ~ dnorm(b1,tau1) beta2[i] ~ dnorm(b2,tau2) tau_big[i] ~ dgamma(s,r) } # Hyperpriors b0 ~ dnorm(180,1/4000) tau0 ~ dnorm(30, 1/9) b1 ~ dnorm(-0.1,1000) tau1 ~ dnorm(1000,1000) b2 ~ dnorm(80,1/100) tau2 ~ dnorm(10,1) s ~ dnorm(7,1) r ~ dnorm(10000, 1/10000) }&quot; # COMPILE y &lt;- full_LiberalArts$Y2018 model_data5 &lt;- as.data.frame(cbind(y, x1 = full_LiberalArts$SAT_AVG_1617, x2 = full_LiberalArts$ADM_RATE_1617)) model_data5 &lt;- na.omit(model_data5) liberal_jags_2 &lt;- jags.model(textConnection(liberal_model_2), data = list(y = model_data5$y, x1 = model_data5$x1, x2 = model_data5$x2), inits=list(.RNG.name = &quot;base::Wichmann-Hill&quot;, .RNG.seed = 454)) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 101 ## Unobserved stochastic nodes: 412 ## Total graph size: 1035 ## ## Initializing model # SIMULATE the model liberal_sim_2 &lt;- coda.samples(liberal_jags_2, variable.names = c(&quot;b0&quot;,&quot;tau0&quot;,&quot;b1&quot;,&quot;tau1&quot;,&quot;b2&quot;,&quot;tau2&quot;,&quot;s&quot;,&quot;r&quot;), n.iter = 10000) # STORE the chains in a data frame liberal_chains_2 &lt;- data.frame(liberal_sim_2[[1]]) 7.3.3 Model summary summary(liberal_sim_2) ## ## Iterations = 1001:11000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## b0 178.4834 0.56301 0.0056301 0.1895714 ## b1 -0.1090 0.02947 0.0002947 0.0039424 ## b2 43.1793 5.95589 0.0595589 4.0299101 ## r 10016.5430 99.18842 0.9918842 1.2203209 ## s 0.1517 0.01632 0.0001632 0.0009056 ## tau0 30.1001 3.00404 0.0300404 0.0510047 ## tau1 999.9994 0.03244 0.0003244 0.0004133 ## tau2 10.0017 1.01154 0.0101154 0.0176700 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## b0 177.2556 178.0642 178.5477 1.789e+02 1.793e+02 ## b1 -0.1706 -0.1287 -0.1086 -8.759e-02 -5.609e-02 ## b2 33.4737 38.3188 42.4722 4.779e+01 5.528e+01 ## r 9819.6765 9950.7472 10016.6172 1.008e+04 1.022e+04 ## s 0.1220 0.1402 0.1511 1.627e-01 1.850e-01 ## tau0 24.2727 28.0611 30.0777 3.214e+01 3.610e+01 ## tau1 999.9360 999.9776 999.9993 1.000e+03 1.000e+03 ## tau2 8.0292 9.3206 9.9837 1.067e+01 1.202e+01 7.3.4 Posterior inference For an unknown Liberal Arts College with student mean SAT score of 1450 and an admission rate of \\(30\\%\\) (e.g. Dvictor college), we could predict its ranking from our rjags simulation. liberal_chains_2 &lt;- liberal_chains_2 %&gt;% mutate(beta0_new = rnorm(10000,b0,(1/tau0)^(1/2))) %&gt;% mutate(beta1_new = rnorm(10000,b1,(1/tau1)^(1/2))) %&gt;% mutate(beta2_new = rnorm(10000,b2,(1/tau2)^(1/2))) %&gt;% mutate(tau_big_new = rgamma(10000,s,r)) %&gt;% mutate(ranking_new = rnorm(10000, mean = beta0_new + beta1_new * 1450 + beta2_new * 0.3, sd = (1/tau_big_new)^(1/2))) liberal_chains_2 %&gt;% summarize(quantile(ranking_new,0.025),quantile(ranking_new,0.975)) ## quantile(ranking_new, 0.025) quantile(ranking_new, 0.975) ## 1 -1515467 1813372 A \\(95\\%\\) credible interval is not pretty! This means that we over-evaluate the variability of a lot of predictors. 7.4 Future steps Given the problem that we encounter at Dvictor College, we also need to pay attention to narrowing the variability in our Bayesian models! "],
["bayesian-models-part-2.html", "Chapter 8 Bayesian Models Part 2", " Chapter 8 Bayesian Models Part 2 How will we simulate what we want to know? HINT: After you start SIMPLE, everything suddenly goes really complicated. After the last checkpoint, we realize that ranking is not the sole factor in students’ choice (sometimes not even an important factor). To accurately represent the school selection process, we find it more appropriate to create a new concept called Fitness Index that measures the level of fitness between the student and schools. We intend to create a Shiny App for students interested in finding colleges. Finding inputing predictive variables, the system will provide a list of top liberal arts colleges and universities. User Input in the Shiny App Description How users input Size of school (UGDS) number of undergraduate degree-seeking students Users can select one or more ranges below: (0,1000); (1000,1800); (1800,2500); (2500+) SAT Scores (SAT_AVG) combined SAT average by year, interchangeable with SAT Users can input their own SAT score. Our system will compare it with average SAT scores of each college and calculate students’ compatibility. (ACT score will be automatically transformed.) A college’s region (REGION) region-wise (e.g. Minnesota is in the Plains region) This is a decisive check-off box. Users can filter away regions where they don’t want to attend college. Setting (LOCALE) setting (large city, small town, etc.) This is a low weight variable where students can state their preference of the geographical location of colleges. Colleges that fit their description will be considered more compatible with students; schools will not be eliminated solely by this factor. Average Cost Per Year (COSTT4_A) average cost of attendance per year Users can use a slidebar to set their preferred full tuition range. Racial diversity (UGDS_WHITE) racial diversity by the percentage of white students Users can select a preferred percentage. Schools with an incompatible percentage (&gt;20% net difference) will be punished by the algorithm; schools will only be eliminated solely by this factor if net difference &gt; 40%. But still, ranking plays an important role in students’ decision making process! We will keep working on predicting rankings and fitness with these in mind. "]
]
