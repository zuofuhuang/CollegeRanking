[
["index.html", "MATH 454 Bayesian Statistics - College Ranking Chapter 1 Preface", " MATH 454 Bayesian Statistics - College Ranking Zuofu Huang and Kavya Shetty, Spring 2019 Macalester College Chapter 1 Preface Hello! Welcome to the bookdown of our MATH 454 Bayesian Statistics Project! We are having a lot of fun doing this project, and you are always welcome to come back at any point for our progress! Credit to xkcd: https://xkcd.com/1236/ "],
["motivation.html", "Chapter 2 Motivation 2.1 Why this? 2.2 A snapshot", " Chapter 2 Motivation 2.1 Why this? Why have Princeton and Williams topped the US News and World Report ranking charts for so long? What factors are considered when we create college rankings? Though reductionary, college rankings are often what parents and prospective students consider in some way when approaching their college search process. Rankings do not often change a great deal and there are many aspects that play into how a ranking is constructed based on what the compiler deems important. In our report, we aim to investigate how these rankings are constructed and eventually provide a specialized tool for students to tailor a college’s “rank” to their own set of criteria. 2.2 A snapshot In our project, we aim to look at the factors that play into college rankings and use our findings to create a specialized ranking tool for students. Factors like region, setting, size, admissions rate, diversity, and standardized testing scores play into the score a given school will receive. "],
["data-source.html", "Chapter 3 Data source 3.1 Data that makes our work possible 3.2 Our data dictionary 3.3 Changes to datasets", " Chapter 3 Data source 3.1 Data that makes our work possible Our dataset was collected from College Scorecard, which is a tool from the College Affordability and Transparency Center. The data is held on data.gov from the Department of Education and maintained by Brian Fu. The last update was September 29, 2017 and our years of interest include 2014-2017. This dataset includes all the aforementioned variables and provides a profile for each school. In order to cross-reference these with ranking data, we scraped data from Andre G. Reiter’s U.S. News &amp; World Report Historical Liberal Arts College and University Rankings datasets, which he compiled from both print and digital records. There were some data missing and we filled that with information compiled on the website Public University Honors. The dataset was called “Average U.S. News Rankings for 123 Universities: 2012-2019” and was posted September 18, 2016 and was most recently updated on September 10, 2018 to include the 2019 rankings. It’s only possible to do what we do because of the awesome data! http://andyreiter.com/datasets/ (A base of University and Liberal Arts ranking datasets) https://publicuniversityhonors.com/2016/09/18/average-u-s-news-rankings-for-126-universities-2010-1017/ (Where we have access to missing University rankings that allow us to input manually) https://catalog.data.gov/dataset/college-scorecard (Three big datasets that founds our analysis) https://collegescorecard.ed.gov/data/documentation/ (Big dataset dictionary) 3.2 Our data dictionary Variable Name Description Institution Name (INSTNM) name of college/university Region (REGION) region-wise (e.g. Minnesota is in the Plains region) Setting (LOCALE) setting (large city, small town, etc.) Size of School (UGDS) number of undergraduate degree-seeking students Admission Rate (ADM_RATE) college’s admission rate SAT Scores (SAT_AVG) combined SAT average by year, interchangeable with ACT Racial Diversity (UGDS_WHITE) racial diversity by percent of white students Average Cost Per Year (COSTT4_A) average cost of attendance per year 3.3 Changes to datasets To ensure the consistency among datasets, we made a few necessary changes to the original datasets. For University ranking datasets: We collected and manually added rankings of 60 universities between 2012 and 2019. For Liberal Arts College ranking datasets: To avoid repetition, we changed Westminster College in Pennsylvania to “Westminster College-PA”; changed Wheaton College in Massachusetts to “Wheaton College-MA”. We deleted US Military Academy, US Air Force Academy, US Naval Academy because the infomration is not disclosed to the public and thus not available for viewing. We deleted Grove City College and Principia College because their information were not available in the mega datasets. We made slight changes on a few colleges, such as “St. Norbert College” to “Saint Norbert College” in order to match with the name in mega datasets. For Mega datasets: To avoid repetition, we changed Westminster College in Pennsylvania to “Westminster College-PA”; changed Wheaton College in Massachusetts to “Wheaton College-MA”. We renamed the unranked St. John’s College by its geographic location; renamed the two unranked Union Colleges by their geographic locations. "],
["data-cleaning.html", "Chapter 4 Data cleaning 4.1 What did we do?", " Chapter 4 Data cleaning 4.1 What did we do? If you don’t want to look at all the lengthy work, Kavya and Zuofu are excited to talk through the process with you! library(readxl) library(dplyr) library(ggplot2) library(rjags) library(forcats) library(gridExtra) library(shiny) library(data.table) library(knitr) 4.1.1 Load data, select and rename columns myData1617 &lt;- read.csv(file = &quot;/Users/kevinhuang/Desktop/Spring 2019/MATH 454/454 Project/CollegeScorecard_Raw_Data/MERGED2016_17_PP.csv&quot;) myData1617_sub &lt;- myData1617 %&gt;% select(INSTNM,REGION,LOCALE,UGDS,ADM_RATE,ACTCMMID,SAT_AVG,UGDS_WHITE,COSTT4_A) names(myData1617_sub) &lt;- c(&quot;INSTNM&quot;, &quot;REGION&quot;,&quot;LOCALE&quot;,&quot;UGDS_1617&quot;,&quot;ADM_RATE_1617&quot;,&quot;ACTCMMID_1617&quot;,&quot;SAT_AVG_1617&quot;,&quot;UGDS_WHITE_1617&quot;,&quot;COSTT4_A_1617&quot;) myData1516 &lt;- read.csv(file = &quot;/Users/kevinhuang/Desktop/Spring 2019/MATH 454/454 Project/CollegeScorecard_Raw_Data/MERGED2015_16_PP.csv&quot;) myData1516_sub &lt;- myData1516 %&gt;% select(INSTNM,UGDS,ADM_RATE,ACTCMMID,SAT_AVG,UGDS_WHITE,COSTT4_A) names(myData1516_sub) &lt;- c(&quot;INSTNM&quot;,&quot;UGDS_1516&quot;,&quot;ADM_RATE_1516&quot;,&quot;ACTCMMID_1516&quot;,&quot;SAT_AVG_1516&quot;,&quot;UGDS_WHITE_1516&quot;,&quot;COSTT4_A_1516&quot;) myData1415 &lt;- read.csv(file = &quot;/Users/kevinhuang/Desktop/Spring 2019/MATH 454/454 Project/CollegeScorecard_Raw_Data/MERGED2014_15_PP.csv&quot;) myData1415_sub &lt;- myData1415 %&gt;% select(INSTNM,UGDS,ADM_RATE,ACTCMMID,SAT_AVG,UGDS_WHITE,COSTT4_A) names(myData1415_sub) &lt;- c(&quot;INSTNM&quot;,&quot;UGDS_1415&quot;,&quot;ADM_RATE_1415&quot;,&quot;ACTCMMID_1415&quot;,&quot;SAT_AVG_1415&quot;,&quot;UGDS_WHITE_1415&quot;,&quot;COSTT4_A_1415&quot;) USNewsUniversity &lt;- read_excel(&quot;/Users/kevinhuang/Desktop/Spring 2019/MATH 454/454 Project/USNewsUniversity.xlsx&quot;) newUniversity &lt;- USNewsUniversity %&gt;% select(c(UniversityName, State, Y2019, Y2018, Y2017, Y2016, Y2015, Y2014, Y2013, Y2012)) %&gt;% na.omit() USNewsLiberalArts &lt;- read_excel(&quot;/Users/kevinhuang/Desktop/Spring 2019/MATH 454/454 Project/USNewsLiberalArts.xlsx&quot;) newLiberalArts &lt;- USNewsLiberalArts %&gt;% select(c(CollegeName, State, Y2019, Y2018, Y2017, Y2016, Y2015, Y2014, Y2013, Y2012)) %&gt;% na.omit() 4.1.2 Filter schools that have a rank college &lt;- rep(NA,149) for (i in 1:149){ college[i] = newLiberalArts[i,]$CollegeName } university &lt;- rep(NA,120) for (i in 1:120){ university[i] = newUniversity[i,]$UniversityName } schools &lt;- c(college, university) # Run tests to make sure that datasets are consistent: check_1_Data1617 &lt;- myData1617_sub %&gt;% filter(INSTNM %in% college) dim(check_1_Data1617) check_3_Data1415 &lt;- myData1415_sub %&gt;% filter(INSTNM %in% college) dim(check_3_Data1415) check_2_Data1516 &lt;- myData1516_sub %&gt;% filter(INSTNM %in% college) dim(check_2_Data1516) # Identify class of each variable, in preparation for transformations. class(myData1617_sub$UGDS_1617) class(myData1617_sub$ADM_RATE_1617) class(myData1617_sub$ACTCMMID_1617) class(myData1617_sub$SAT_AVG_1617) class(myData1617_sub$UGDS_WHITE_1617) class(myData1617_sub$COSTT4_A_1617) class(myData1415_sub$INSTNM) myData1415_sub_characterCollege &lt;- myData1415_sub %&gt;% filter(INSTNM %in% college) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1415 = as.numeric(as.character(UGDS_1415))) %&gt;% mutate(UGDS_WHITE_1415 = as.numeric(as.character(UGDS_WHITE_1415))) %&gt;% mutate(SAT_AVG_1415 = as.numeric(as.character(SAT_AVG_1415))) %&gt;% mutate(ACTCMMID_1415 = as.numeric(as.character(ACTCMMID_1415))) %&gt;% mutate(ADM_RATE_1415 = as.numeric(as.character(ADM_RATE_1415))) %&gt;% mutate(COSTT4_A_1415 = as.numeric(as.character(COSTT4_A_1415))) myData1516_sub_characterCollege &lt;- myData1516_sub %&gt;% filter(INSTNM %in% college) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1516 = as.numeric(as.character(UGDS_1516))) %&gt;% mutate(UGDS_WHITE_1516 = as.numeric(as.character(UGDS_WHITE_1516))) %&gt;% mutate(SAT_AVG_1516 = as.numeric(as.character(SAT_AVG_1516))) %&gt;% mutate(ACTCMMID_1516 = as.numeric(as.character(ACTCMMID_1516))) %&gt;% mutate(ADM_RATE_1516 = as.numeric(as.character(ADM_RATE_1516))) %&gt;% mutate(COSTT4_A_1516 = as.numeric(as.character(COSTT4_A_1516))) myData1617_sub_characterCollege &lt;- myData1617_sub %&gt;% filter(INSTNM %in% college) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1617 = as.numeric(as.character(UGDS_1617))) %&gt;% mutate(UGDS_WHITE_1617 = as.numeric(as.character(UGDS_WHITE_1617))) %&gt;% mutate(SAT_AVG_1617 = as.numeric(as.character(SAT_AVG_1617))) %&gt;% mutate(ACTCMMID_1617 = as.numeric(as.character(ACTCMMID_1617))) %&gt;% mutate(ADM_RATE_1617 = as.numeric(as.character(ADM_RATE_1617))) %&gt;% mutate(COSTT4_A_1617 = as.numeric(as.character(COSTT4_A_1617))) # To recheck that all three datasets are the same with regard to college names. dim(myData1415_sub_characterCollege) dim(myData1516_sub_characterCollege) dim(myData1617_sub_characterCollege) 4.1.3 Join datasets fullLiberalArts &lt;- newLiberalArts %&gt;% full_join(myData1415_sub_characterCollege, by = c(&quot;CollegeName&quot; = &quot;INSTNM&quot;)) %&gt;% full_join(myData1516_sub_characterCollege, by = c(&quot;CollegeName&quot; = &quot;INSTNM&quot;)) %&gt;% full_join(myData1617_sub_characterCollege, by = c(&quot;CollegeName&quot; = &quot;INSTNM&quot;)) dim(fullLiberalArts) ## [1] 175 30 full_LiberalArts &lt;- fullLiberalArts[!duplicated(fullLiberalArts$CollegeName), ] dim(full_LiberalArts) # After we removed the repetitive entries. ## [1] 149 30 myData1415_sub_characterUniversity &lt;- myData1415_sub %&gt;% filter(INSTNM %in% university) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1415 = as.numeric(as.character(UGDS_1415))) %&gt;% mutate(UGDS_WHITE_1415 = as.numeric(as.character(UGDS_WHITE_1415))) %&gt;% mutate(SAT_AVG_1415 = as.numeric(as.character(SAT_AVG_1415))) %&gt;% mutate(ACTCMMID_1415 = as.numeric(as.character(ACTCMMID_1415))) %&gt;% mutate(ADM_RATE_1415 = as.numeric(as.character(ADM_RATE_1415))) %&gt;% mutate(COSTT4_A_1415 = as.numeric(as.character(COSTT4_A_1415))) myData1516_sub_characterUniversity &lt;- myData1516_sub %&gt;% filter(INSTNM %in% university) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1516 = as.numeric(as.character(UGDS_1516))) %&gt;% mutate(UGDS_WHITE_1516 = as.numeric(as.character(UGDS_WHITE_1516))) %&gt;% mutate(SAT_AVG_1516 = as.numeric(as.character(SAT_AVG_1516))) %&gt;% mutate(ACTCMMID_1516 = as.numeric(as.character(ACTCMMID_1516))) %&gt;% mutate(ADM_RATE_1516 = as.numeric(as.character(ADM_RATE_1516))) %&gt;% mutate(COSTT4_A_1516 = as.numeric(as.character(COSTT4_A_1516))) myData1617_sub_characterUniversity &lt;- myData1617_sub %&gt;% filter(INSTNM %in% university) %&gt;% mutate(INSTNM = as.character(INSTNM)) %&gt;% mutate(UGDS_1617 = as.numeric(as.character(UGDS_1617))) %&gt;% mutate(UGDS_WHITE_1617 = as.numeric(as.character(UGDS_WHITE_1617))) %&gt;% mutate(SAT_AVG_1617 = as.numeric(as.character(SAT_AVG_1617))) %&gt;% mutate(ACTCMMID_1617 = as.numeric(as.character(ACTCMMID_1617))) %&gt;% mutate(ADM_RATE_1617 = as.numeric(as.character(ADM_RATE_1617))) %&gt;% mutate(COSTT4_A_1617 = as.numeric(as.character(COSTT4_A_1617))) fullUniversity &lt;- newUniversity %&gt;% full_join(myData1415_sub_characterUniversity, by = c(&quot;UniversityName&quot; = &quot;INSTNM&quot;)) %&gt;% full_join(myData1516_sub_characterUniversity, by = c(&quot;UniversityName&quot; = &quot;INSTNM&quot;)) %&gt;% full_join(myData1617_sub_characterUniversity, by = c(&quot;UniversityName&quot; = &quot;INSTNM&quot;)) dim(fullUniversity) ## [1] 120 30 "],
["visualization.html", "Chapter 5 Visualization 5.1 1st &amp; 2nd Peeks 5.2 3rd Peek 5.3 4th Peek", " Chapter 5 Visualization Intuitively, what’s going on? 5.1 1st &amp; 2nd Peeks In our first and second peeks, we are interested in geographical locations of universities and colleges. It is surprising to learn that students, when choosing schools, give much weight to a school’s geographical location. We investigate two variables that contribute to prospective students’ choice, giving side-by-side plots that reflect how the variables vary between universities and liberal arts colleges. REGION_collapse &lt;- fct_collapse(as.character(full_LiberalArts$REGION), NewEngland = &quot;1&quot;, MidEast = &quot;2&quot;, GreatLakes = &quot;3&quot;, Plains = &quot;4&quot;, Southeast = &quot;5&quot;, Southwest = &quot;6&quot;, RockyMountains = &quot;7&quot;, FarWest = &quot;8&quot;) full_LiberalArts$REGION_collapse &lt;- REGION_collapse LOCALE_collapse_lac1 &lt;- fct_collapse(full_LiberalArts$LOCALE, City = c(&quot;11&quot;,&quot;12&quot;,&quot;13&quot;), Suburb = c(&quot;21&quot;,&quot;22&quot;,&quot;23&quot;), Town = c(&quot;31&quot;,&quot;32&quot;,&quot;33&quot;), Rural = c(&quot;41&quot;,&quot;42&quot;,&quot;43&quot;)) LOCALE_collapse_lac2 &lt;- fct_collapse(full_LiberalArts$LOCALE, LargeCity = &quot;11&quot;, MidsizeCity = &quot;12&quot;, SmallCity = &quot;13&quot;, LargeSuburb = &quot;21&quot;, MidsizeSuburb = &quot;22&quot;, SmallSuburb = &quot;23&quot;, FringeTown = &quot;31&quot;, DistantTown = &quot;32&quot;, RemoteTown = &quot;33&quot;, FringeRural = &quot;41&quot;, DistantRural = &quot;42&quot;, RemoteTown = &quot;43&quot;) LOCALE_collapse_lac3 &lt;- fct_collapse(full_LiberalArts$LOCALE, &quot;3&quot; = c(&quot;11&quot;,&quot;12&quot;,&quot;13&quot;), &quot;2&quot; = c(&quot;21&quot;,&quot;22&quot;,&quot;23&quot;), &quot;1&quot; = c(&quot;31&quot;,&quot;32&quot;,&quot;33&quot;), &quot;0&quot; = c(&quot;41&quot;,&quot;42&quot;,&quot;43&quot;)) full_LiberalArts$LOCALE_collapse_lac1 &lt;- LOCALE_collapse_lac1 full_LiberalArts$LOCALE_collapse_lac2 &lt;- LOCALE_collapse_lac2 full_LiberalArts$LOCALE_collapse_lac3 &lt;- as.numeric(as.character(LOCALE_collapse_lac3)) REGION_collapse &lt;- fct_collapse(as.character(fullUniversity$REGION), NewEngland = &quot;1&quot;, MidEast = &quot;2&quot;, GreatLakes = &quot;3&quot;, Plains = &quot;4&quot;, Southeast = &quot;5&quot;, Southwest = &quot;6&quot;, RockyMountains = &quot;7&quot;, FarWest = &quot;8&quot;) fullUniversity$REGION_collapse &lt;- REGION_collapse LOCALE_collapse &lt;- fct_collapse(fullUniversity$LOCALE, City = c(&quot;11&quot;,&quot;12&quot;,&quot;13&quot;), Suburb = c(&quot;21&quot;,&quot;22&quot;,&quot;23&quot;), Town = c(&quot;31&quot;,&quot;32&quot;,&quot;33&quot;), Rural = c(&quot;41&quot;,&quot;42&quot;,&quot;43&quot;)) LOCALE_collapse2 &lt;- fct_collapse(fullUniversity$LOCALE, LargeCity = &quot;11&quot;, MidsizeCity = &quot;12&quot;, SmallCity = &quot;13&quot;, LargeSuburb = &quot;21&quot;, MidsizeSuburb = &quot;22&quot;, SmallSuburb = &quot;23&quot;, FringeTown = &quot;31&quot;, DistantTown = &quot;32&quot;, RemoteTown = &quot;33&quot;, FringeRural = &quot;41&quot;, DistantRural = &quot;42&quot;, RemoteTown = &quot;43&quot;) LOCALE_collapse3 &lt;- fct_collapse(fullUniversity$LOCALE, &quot;3&quot; = c(&quot;11&quot;,&quot;12&quot;,&quot;13&quot;), &quot;2&quot; = c(&quot;21&quot;,&quot;22&quot;,&quot;23&quot;), &quot;1&quot; = c(&quot;31&quot;,&quot;32&quot;,&quot;33&quot;), &quot;0&quot; = c(&quot;41&quot;,&quot;42&quot;,&quot;43&quot;)) fullUniversity$LOCALE_collapse &lt;- LOCALE_collapse fullUniversity$LOCALE_collapse2 &lt;- LOCALE_collapse2 fullUniversity$LOCALE_collapse3 &lt;- as.numeric(as.character(LOCALE_collapse3)) g1 &lt;- ggplot(fullUniversity, aes(x = REGION_collapse, fill = LOCALE_collapse)) + geom_bar() + xlab(&quot;Region&quot;) + scale_fill_discrete(name = &quot;Breakdown&quot;) + ylab(&quot;Count&quot;) + ggtitle(&quot;Region of Universities, broken down by location&quot;) + theme(axis.text.x = element_text(angle = 30, hjust = 1)) g2 &lt;- ggplot(full_LiberalArts, aes(x = REGION_collapse, fill = LOCALE_collapse_lac1)) + geom_bar() + xlab(&quot;Region&quot;) + scale_fill_discrete(name = &quot;Breakdown&quot;) + ylab(&quot;Count&quot;) + ggtitle(&quot;Region of Liberal Arts Colleges, broken down by location&quot;) + theme(axis.text.x = element_text(angle = 30, hjust = 1)) grid.arrange(g1, g2, nrow = 2) g3 &lt;- ggplot(fullUniversity, aes(x = LOCALE_collapse, fill = LOCALE_collapse2)) + geom_bar() + xlab(&quot;Geographical Location&quot;) + scale_fill_discrete(name = &quot;Breakdown&quot;) + ylab(&quot;Count&quot;) + ggtitle(&quot;Location of Universities, broken down by size&quot;) + theme(plot.title = element_text(size = 10)) g4 &lt;- ggplot(full_LiberalArts, aes(x = LOCALE_collapse_lac1, fill = LOCALE_collapse_lac2)) + geom_bar() + xlab(&quot;Geographical Location&quot;) + scale_fill_discrete(name = &quot;Breakdown&quot;) + ylab(&quot;Count&quot;) + ggtitle(&quot;Location of Liberal Arts Colleges, broken down by size&quot;) + theme(plot.title = element_text(size = 9.5)) + theme(axis.text.x = element_text(angle = 30)) grid.arrange(g3, g4, nrow = 1) 5.2 3rd Peek “In a utilitarian age, of all other times, it is a matter of grave importance that fairy tales should be respected.” -Charles Dickens Here’s when people tend to (have the excuse to) get a little more utilitarian. Studies find that people pay more attention to ranking when ranking data is available. We (or more precisely, Zuofu) are just like that. SAT is usually one of the major indicators of students’ fitness with certain colleges, both from students’ and colleges’ perspective. We do a bi-variate visualization between average SAT score of 2016-2017 academic year and university ranking of Year 2018. If you look further in our project (which hopefully you do!), you will see this is the basis of our first introductory model. p1 &lt;- ggplot(fullUniversity, aes(x = SAT_AVG_1617, y = as.numeric(Y2018))) + geom_point() + geom_smooth(method = &quot;lm&quot;) + xlim(1000,1600) + ylim(1,150) + xlab(&quot;Average SAT Score&quot;) + ylab(&quot;School Ranking 2018&quot;) + ggtitle(&quot;Universities&quot;) p2 &lt;- ggplot(full_LiberalArts, aes(x = SAT_AVG_1617, y = as.numeric(Y2018))) + geom_point() + geom_smooth(method = &quot;lm&quot;) + xlim(1000,1600) + ylim(1,150) + xlab(&quot;Average SAT Score&quot;) + ylab(&quot;School Ranking 2018&quot;) + ggtitle(&quot;Liberal Arts Colleges&quot;) grid.arrange(p1, p2, nrow = 1) 5.3 4th Peek “Diversity: the art of thinking independently together.” -Malcolm Forbes In an effort to create a more just and inclusive world, the opportunity to engage with people of different backgrounds and identities plays an increasingly important role. We plan to look into the common trend between the size of the student body and diversity factors (namely, the precentage of white students and the cost of attending college). fullUniversity$UGDS_WHITE_1617_categ = cut(fullUniversity$UGDS_WHITE_1617, c(0,0.2,0.4,0.6,0.8,1)) ggplot(fullUniversity, aes(x = UGDS_1617, y = UGDS_WHITE_1617, color = COSTT4_A_1617)) + geom_point() + xlab(&quot;Size of Undergraduate Student Body&quot;) + scale_fill_discrete(name = &quot;Cost for Year 2017&quot;) + ylab(&quot;Percentage of White Students&quot;) + ggtitle(&quot;Economic and Racial Diversity of Universities&quot;) We didn’t see an observable relationship between cost and the percentage of white students; we did observe that universities of a smaller size tend to correlate with a much larger cost. full_LiberalArts$UGDS_WHITE_1617_categ = cut(full_LiberalArts$UGDS_WHITE_1617, c(0,0.2,0.4,0.6,0.8,1)) ggplot(full_LiberalArts, aes(x = UGDS_1617, y = UGDS_WHITE_1617, color = COSTT4_A_1617)) + geom_point() + xlab(&quot;Size of Undergraduate Student Body&quot;) + scale_fill_discrete(name = &quot;Cost for Year 2017&quot;) + ylab(&quot;Percentage of White Students&quot;) + ggtitle(&quot;Economic and Racial Diversity of Liberal Arts Colleges&quot;) ## Warning: Removed 2 rows containing missing values (geom_point). In terms of liberal arts colleges, we can see that the data points (schools) are more clustered at the top section of the plot. This makes sense to us given that most liberal arts colleges are private compared to universities. We didn’t see an observable relationship between cost and the percentage of white students "],
["bayesian-models-part-1-1-university.html", "Chapter 6 Bayesian Models Part 1.1 University 6.1 Model 0 6.2 Model 1 6.3 Model 2 6.4 Future steps", " Chapter 6 Bayesian Models Part 1.1 University How will we simulate what we want to know? HINT: Start SIMPLE 6.1 Model 0 Ranking by one year of SAT score: with intuition from \\(\\text{hist}(\\sqrt{1/rgamma(10000,a,b)})\\). 6.1.1 First Impression It appears to be a linear relationship! 6.1.2 Building the model Let \\(Y_i\\) denote the predicted ranking of a university in Year 2018. \\(Y_i\\) is predicted by \\[X_i = \\text{student mean SAT score of Year 2016-17}\\] Our model can be written as: \\[\\begin{align} Y_i &amp; \\sim N(\\beta_0 + \\beta_1X_i,\\tau_0) \\\\ \\beta_0 &amp; \\sim N(300,250000^{-1}) \\\\ \\beta_1 &amp; \\sim N(0,100^{-1}) \\\\ \\tau_0 &amp; \\sim Gamma(7,4000) \\end{align}\\] # DEFINE the model university_model_0 &lt;- &quot;model{ for(i in 1:length(y)) { # Data model y[i] ~ dnorm(beta0 + beta1 * x[i], tau0) } # Priors for theta beta0 ~ dnorm(300,1/250000) beta1 ~ dnorm(0, 1/100) tau0 ~ dgamma(7,4000) }&quot; # COMPILE the model model_data0 &lt;- data.frame(y = fullUniversity$Y2018, x = fullUniversity$SAT_AVG_1617) model_data0 &lt;- na.omit(model_data0) university_jags_0 &lt;- jags.model(textConnection(university_model_0), data = list(y = model_data0$y, x = model_data0$x), inits = list(.RNG.name = &quot;base::Wichmann-Hill&quot;, .RNG.seed = 454)) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 114 ## Unobserved stochastic nodes: 3 ## Total graph size: 440 ## ## Initializing model # SIMULATE the model university_sim_0 &lt;- coda.samples(university_jags_0, variable.names = c(&quot;beta0&quot;, &quot;beta1&quot;, &quot;tau0&quot;), n.iter = 10000) # STORE the chains in a data frame university_chains_0 &lt;- data.frame(university_sim_0[[1]]) 6.1.3 Model summary summary(university_sim_0) ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## beta0 403.411563 3.227e+01 3.227e-01 4.714e+00 ## beta1 -0.263423 2.472e-02 2.472e-04 3.617e-03 ## tau0 0.002421 3.279e-04 3.279e-06 1.303e-05 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## beta0 355.59133 390.793008 405.743679 421.198553 447.191343 ## beta1 -0.29699 -0.277127 -0.265219 -0.253713 -0.226779 ## tau0 0.00182 0.002212 0.002415 0.002628 0.003062 6.1.4 Posterior inference For an unknown university with a mean student SAT score of 1450 (e.g. Bvictor University), we could predict its ranking from our rjags simulation. university_chains_0 &lt;- university_chains_0 %&gt;% mutate(ranking_new = rnorm(10000, mean = beta0 + beta1*1450, sd = (1/tau0)^(1/2))) university_chains_0 %&gt;% summarize(quantile(ranking_new,0.025),quantile(ranking_new,0.975)) ## quantile(ranking_new, 0.025) quantile(ranking_new, 0.975) ## 1 -19.49469 62.42904 A \\(95\\%\\) credible interval is \\((-20,62)\\). Due to restrictions of this one-predictor model, we expect to see an improvement in later models. 6.2 Model 1 Ranking by three years of SAT score. By analysis, the three predictors are multicollinear, meaning that this may not be a significantly better model than Model 0. 6.2.1 First Impression It’s harder to make predictions (and as it turns out, hard to make visualizations as well) with three years’ SAT score in the same linear model, as we can foresee significant multicollinearity. This is clearly seen that the coefficient for SAT average of Year 2014-15 (SAT_AVG_1415) variable is positive, meaning that school ranking worsens as the average SAT score increases. To further test our thinking, we created a linear model university_linear_1. From the ANOVA summary, we see that any one of these variables should be sufficient. university_linear_1 &lt;- lm(as.numeric(Y2018) ~ SAT_AVG_1617 + SAT_AVG_1516 + SAT_AVG_1415, fullUniversity) summary(university_linear_1) ## ## Call: ## lm(formula = as.numeric(Y2018) ~ SAT_AVG_1617 + SAT_AVG_1516 + ## SAT_AVG_1415, data = fullUniversity) ## ## Residuals: ## Min 1Q Median 3Q Max ## -104.127 -8.988 -2.062 12.129 38.789 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 415.49873 20.70087 20.072 &lt;2e-16 *** ## SAT_AVG_1617 -0.04769 0.14485 -0.329 0.743 ## SAT_AVG_1516 -0.26890 0.18977 -1.417 0.159 ## SAT_AVG_1415 0.04301 0.16163 0.266 0.791 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.68 on 110 degrees of freedom ## (6 observations deleted due to missingness) ## Multiple R-squared: 0.7292, Adjusted R-squared: 0.7218 ## F-statistic: 98.75 on 3 and 110 DF, p-value: &lt; 2.2e-16 anova(university_linear_1) ## Analysis of Variance Table ## ## Response: as.numeric(Y2018) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## SAT_AVG_1617 1 113627 113627 293.4484 &lt;2e-16 *** ## SAT_AVG_1516 1 1052 1052 2.7173 0.1021 ## SAT_AVG_1415 1 27 27 0.0708 0.7907 ## Residuals 110 42594 387 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For the sake of scientific experiment (and to catch the audience’s interest), we still did a Bayesian model and Bayesian posterior inference. We found out that this model does a slightly better job at predicting college rankings (by eliminating negative rankings) because it naturally contains more information as we increase the number of predictors. The change is not significant enough to call it an improvement. 6.2.2 Building the model Similarly, we construct a linear regression model of \\[Y_i = \\text{the predicted 2018 ranking of a university}\\] by \\[\\begin{align} X_{1i} &amp; = \\text{student mean SAT score of Year 2014-15} \\\\ X_{2i} &amp; = \\text{student mean SAT score of Year 2015-16} \\\\ X_{3i} &amp; = \\text{student mean SAT score of Year 2016-17} \\end{align}\\] \\[\\begin{align} Y_i &amp; \\sim N(\\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\beta_3X_{3i},\\tau_{\\text{big}}) \\\\ \\beta_0 &amp; \\sim N(0,250000^{-1}) \\\\ \\beta_1 &amp; \\sim N(0,0.01^{-1}) \\\\ \\beta_2 &amp; \\sim N(0,0.01^{-1}) \\\\ \\beta_3 &amp; \\sim N(0,0.01^{-1}) \\\\ \\tau_{\\text{big}} &amp; \\sim Gamma(7,1000) \\end{align}\\] university_model_1 &lt;- &quot;model{ # Data: observations for(i in 1:length(y)) { y[i] ~ dnorm(beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x3[i], tau_big) } # Data: subjects beta0 ~ dnorm(0,1/250000) beta1 ~ dnorm(0,100) beta2 ~ dnorm(0,100) beta3 ~ dnorm(0,100) tau_big ~ dgamma(7,1000) }&quot; # COMPILE y &lt;- fullUniversity$Y2018 model_data1 &lt;- as.data.frame(cbind(y, x1 = fullUniversity$SAT_AVG_1415, x2 = fullUniversity$SAT_AVG_1516, x3 = fullUniversity$SAT_AVG_1617)) model_data1 &lt;- na.omit(model_data1) university_jags_1 &lt;- jags.model(textConnection(university_model_1), data = list(y = model_data1$y, x1 = model_data1$x1, x2 = model_data1$x2, x3 = model_data1$x3), inits=list(.RNG.name = &quot;base::Wichmann-Hill&quot;, .RNG.seed = 454)) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 114 ## Unobserved stochastic nodes: 5 ## Total graph size: 876 ## ## Initializing model # SIMULATE the model university_sim_1 &lt;- coda.samples(university_jags_1, variable.names = c(&quot;beta0&quot;,&quot;beta1&quot;,&quot;beta2&quot;,&quot;beta3&quot;,&quot;tau_big&quot;), n.iter = 10000) # STORE the chains in a data frame university_chains_1 &lt;- data.frame(university_sim_1[[1]]) 6.2.3 Model summary summary(university_sim_1) ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## beta0 409.864989 3.553e+01 3.553e-01 5.783e+00 ## beta1 0.001373 1.078e-01 1.078e-03 1.120e-01 ## beta2 -0.179656 7.938e-02 7.938e-04 4.684e-02 ## beta3 -0.090932 7.417e-02 7.417e-04 5.086e-02 ## tau_big 0.002781 4.008e-04 4.008e-06 2.714e-05 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## beta0 345.977738 400.686962 414.807439 427.055384 447.427911 ## beta1 -0.224077 -0.074142 0.033869 0.084271 0.127709 ## beta2 -0.304552 -0.247353 -0.181364 -0.127231 -0.006660 ## beta3 -0.212426 -0.151206 -0.106569 -0.025275 0.034454 ## tau_big 0.002051 0.002538 0.002778 0.003029 0.003556 6.2.4 Posterior inference For an unknown university with three years’ mean student SAT score of 1450, 1440, 1420 (e.g. Cvictor University), we could predict its ranking from our rjags simulation. university_chains_1 &lt;- university_chains_1 %&gt;% mutate(ranking_new = rnorm(10000, mean = beta0 + beta1*1450 + beta2*1440 +beta3*1420, sd = (1/tau_big)^(1/2))) university_chains_1 %&gt;% summarize(quantile(ranking_new,0.025),quantile(ranking_new,0.975)) ## quantile(ranking_new, 0.025) quantile(ranking_new, 0.975) ## 1 -14.28604 62.83153 A \\(95\\%\\) credible interval is \\((-14,64)\\). The \\(95\\%\\) credible interval in the new model does a better job at eliminating negative rankings, which are impossible in reality. 6.3 Model 2 2018 U.S. News Ranking by average SAT score and admissions rate of Year 2017. 6.3.1 First Impression There’s a strong positive relationship between the admission rate of colleges and college ranking (a larger number in college ranking is worse). From color gradient of points, we can see that schools with higher SAT averages tend to be better ranked. 6.3.2 Building the model 6.3.2.1 Step 1 A linear model provides some intuition about how the priors might be constructed. summary(lm(fullUniversity$Y2018 ~ fullUniversity$SAT_AVG_1617 + fullUniversity$ADM_RATE_1617)) ## ## Call: ## lm(formula = fullUniversity$Y2018 ~ fullUniversity$SAT_AVG_1617 + ## fullUniversity$ADM_RATE_1617) ## ## Residuals: ## Min 1Q Median 3Q Max ## -78.978 -9.142 -1.986 8.865 43.380 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 172.5262 45.1297 3.823 0.000218 *** ## fullUniversity$SAT_AVG_1617 -0.1150 0.0301 -3.822 0.000219 *** ## fullUniversity$ADM_RATE_1617 84.8854 14.8094 5.732 8.66e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.42 on 111 degrees of freedom ## (6 observations deleted due to missingness) ## Multiple R-squared: 0.7858, Adjusted R-squared: 0.7819 ## F-statistic: 203.6 on 2 and 111 DF, p-value: &lt; 2.2e-16 6.3.2.2 Step 2 Next, we construct a hierarchical model (because we just learned this and wanted to show off) of \\[Y_i = \\text{the predicted 2018 ranking of a university}\\] by \\[\\begin{align} X_{1i} &amp; = \\text{student mean SAT score of Year 2016-17} \\\\ X_{2i} &amp; = \\text{admissions rate during Year 2016-17} \\end{align}\\] \\[\\begin{align} Y_i &amp; \\sim N(\\beta_{0i} + \\beta_{1i}X_{1i} + \\beta_{2i}X_{2i},\\tau_{\\text{big},i}) \\\\ \\beta_{0i} &amp; \\sim N(b_0,\\tau_0) \\\\ \\beta_{1i} &amp; \\sim N(b_1,\\tau_1) \\\\ \\beta_{2i} &amp; \\sim N(b_2,\\tau_2) \\\\ \\tau_{\\text{big},i} &amp; \\sim Gamma(s,r) \\\\ b_0 &amp; \\sim N(180,4000^{-1}) \\\\ \\tau_0 &amp; \\sim N(30, 1/9) \\\\ b_1 &amp; \\sim N(-0.1,0.001^{-1}) \\\\ \\tau_1 &amp; \\sim N(1000, 0.001^{-1}) \\\\ b_2 &amp; \\sim N(80,100^{-1}) \\\\ \\tau_2 &amp; \\sim N(10, 1) \\\\ s &amp; \\sim N(7,1) \\\\ r &amp; \\sim N(10000, 10000^{-1}) \\end{align}\\] university_model_2 &lt;- &quot;model{ # Data: observations for(i in 1:length(y)) { y[i] ~ dnorm(beta0[i] + beta1[i]*x1[i] + beta2[i]*x2[i], tau_big[i]) # Data: subjects beta0[i] ~ dnorm(b0, tau0) beta1[i] ~ dnorm(b1,tau1) beta2[i] ~ dnorm(b2,tau2) tau_big[i] ~ dgamma(s,r) } # Hyperpriors b0 ~ dnorm(180,1/4000) tau0 ~ dnorm(30, 1/9) b1 ~ dnorm(-0.1,1000) tau1 ~ dnorm(1000,1000) b2 ~ dnorm(80,1/100) tau2 ~ dnorm(10,1) s ~ dnorm(7,1) r ~ dnorm(10000, 1/10000) }&quot; # COMPILE y &lt;- fullUniversity$Y2018 model_data2 &lt;- as.data.frame(cbind(y, x1 = fullUniversity$SAT_AVG_1617, x2 = fullUniversity$ADM_RATE_1617)) model_data2 &lt;- na.omit(model_data2) university_jags_2 &lt;- jags.model(textConnection(university_model_2), data = list(y = model_data2$y, x1 = model_data2$x1, x2 = model_data2$x2), inits=list(.RNG.name = &quot;base::Wichmann-Hill&quot;, .RNG.seed = 454)) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 114 ## Unobserved stochastic nodes: 464 ## Total graph size: 1165 ## ## Initializing model # SIMULATE the model university_sim_2 &lt;- coda.samples(university_jags_2, variable.names = c(&quot;b0&quot;,&quot;tau0&quot;,&quot;b1&quot;,&quot;tau1&quot;,&quot;b2&quot;,&quot;tau2&quot;,&quot;s&quot;,&quot;r&quot;), n.iter = 10000) # STORE the chains in a data frame university_chains_2 &lt;- data.frame(university_sim_2[[1]]) 6.3.3 Model summary summary(university_sim_2) ## ## Iterations = 1001:11000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## b0 181.003 0.866642 8.666e-03 0.4845152 ## b1 -0.122 0.003986 3.986e-05 0.0001962 ## b2 86.978 2.748797 2.749e-02 2.0167560 ## r 9978.437 98.495695 9.850e-01 1.3555384 ## s 9.367 0.854744 8.547e-03 0.0428560 ## tau0 29.903 3.051727 3.052e-02 0.0544687 ## tau1 1000.000 0.031393 3.139e-04 0.0003934 ## tau2 10.021 1.000127 1.000e-02 0.0179231 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## b0 179.2258 180.2942 181.163 181.6543 182.5227 ## b1 -0.1298 -0.1247 -0.122 -0.1193 -0.1143 ## b2 80.6675 85.0648 87.714 89.2203 90.5414 ## r 9784.6197 9912.1376 9978.523 10044.9089 10170.4126 ## s 7.7114 8.7880 9.345 9.9331 11.0906 ## tau0 23.8816 27.8340 29.879 31.9392 35.9038 ## tau1 999.9366 999.9792 1000.000 1000.0208 1000.0615 ## tau2 8.0599 9.3477 10.030 10.6819 12.0006 6.3.4 Posterior inference For an unknown university with student mean SAT score of 1450 and an admission rate of \\(30\\%\\) (e.g. Dvictor University), we could predict its ranking from our rjags simulation. university_chains_2 &lt;- university_chains_2 %&gt;% mutate(beta0_new = rnorm(10000,b0,(1/tau0)^(1/2))) %&gt;% mutate(beta1_new = rnorm(10000,b1,(1/tau1)^(1/2))) %&gt;% mutate(beta2_new = rnorm(10000,b2,(1/tau2)^(1/2))) %&gt;% mutate(tau_big_new = rgamma(10000,s,r)) %&gt;% mutate(ranking_new = rnorm(10000, mean = beta0_new + beta1_new * 1450 + beta2_new * 0.3, sd = (1/tau_big_new)^(1/2))) university_chains_2 %&gt;% summarize(quantile(ranking_new,0.025),quantile(ranking_new,0.975)) ## quantile(ranking_new, 0.025) quantile(ranking_new, 0.975) ## 1 -84.91896 143.755 A \\(95\\%\\) credible interval is \\((-86,142)\\). The \\(95\\%\\) credible interval does a poor job at eliminating negative rankings or making an accurate prediction. We propose that this may be due to the variability of the priors and hyperpriors set in our Hierarchical Model. 6.4 Future steps Next, we plan to incorporate more predictive variables of ranking \\(y_i\\). Since some predictors we choose are correlated, we will also emphasize on reflecting that in our models. "],
["bayesian-models-part-1-2-liberal-arts.html", "Chapter 7 Bayesian Models Part 1.2 Liberal Arts 7.1 Model 0 7.2 Model 1 7.3 Model 2 7.4 Future steps", " Chapter 7 Bayesian Models Part 1.2 Liberal Arts The real question: why do we repeat the process on liberal arts colleges again? Answer: Liberal arts colleges fall under an entirely separate ranking list and often have different characteristics (e.g. size and location, private vs public). We repeat the process to observe the similarity and differences between universities and liberal arts colleges. 7.1 Model 0 Ranking by one year of SAT score: with intuition from \\(\\text{hist}(\\sqrt{1/rgamma(10000,a,b)})\\). 7.1.1 First Impression It appears to be a linear relationship! 7.1.2 Building the model Let \\(Y_i\\) denote the predicted ranking of a liberal arts college in Year 2018. \\(Y_i\\) is predicted by \\[X_i = \\text{student mean SAT score of Year 2016-17}\\] Our model can be written as: \\[\\begin{align} Y_i &amp; \\sim N(\\beta_0 + \\beta_1X_i,\\tau_0) \\\\ \\beta_0 &amp; \\sim N(300,250000^{-1}) \\\\ \\beta_1 &amp; \\sim N(0,100^{-1}) \\\\ \\tau_0 &amp; \\sim Gamma(7,4000) \\end{align}\\] # DEFINE the model liberal_model_0 &lt;- &quot;model{ for(i in 1:length(y)) { # Data model y[i] ~ dnorm(beta0 + beta1 * x[i], tau0) } # Priors for theta beta0 ~ dnorm(300,1/250000) beta1 ~ dnorm(0, 1/100) tau0 ~ dgamma(7,4000) }&quot; # COMPILE the model model_data4 &lt;- data.frame(y = full_LiberalArts$Y2018, x = full_LiberalArts$SAT_AVG_1617) model_data4 &lt;- na.omit(model_data4) liberal_jags_0 &lt;- jags.model(textConnection(liberal_model_0), data = list(y = model_data4$y, x = model_data4$x), inits = list(.RNG.name = &quot;base::Wichmann-Hill&quot;, .RNG.seed = 454)) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 101 ## Unobserved stochastic nodes: 3 ## Total graph size: 396 ## ## Initializing model # SIMULATE the model liberal_sim_0 &lt;- coda.samples(liberal_jags_0, variable.names = c(&quot;beta0&quot;, &quot;beta1&quot;, &quot;tau0&quot;), n.iter = 10000) # STORE the chains in a data frame liberal_chains_0 &lt;- data.frame(liberal_sim_0[[1]]) 7.1.3 Model summary summary(liberal_sim_0) ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## beta0 -0.614922 1.583e+01 1.583e-01 2.144e+00 ## beta1 0.021724 1.296e-02 1.296e-04 1.767e-03 ## tau0 0.003849 5.113e-04 5.113e-06 5.914e-06 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## beta0 -31.655750 -11.608615 -1.094746 10.751901 29.460108 ## beta1 -0.003149 0.012368 0.022064 0.030707 0.047298 ## tau0 0.002903 0.003495 0.003829 0.004177 0.004907 7.1.4 Posterior inference For an unknown liberal arts college with a mean student SAT score of 1450 (e.g. Bvictor College), we could predict its ranking from our rjags simulation. liberal_chains_0 &lt;- liberal_chains_0 %&gt;% mutate(ranking_new = rnorm(10000, mean = beta0 + beta1*1450, sd = (1/tau0)^(1/2))) liberal_chains_0 %&gt;% summarize(quantile(ranking_new,0.025),quantile(ranking_new,0.975)) ## quantile(ranking_new, 0.025) quantile(ranking_new, 0.975) ## 1 -1.344388 64.10182 A \\(95\\%\\) credible interval is \\((-1,63)\\). Due to restrictions of this one-predictor model, we expect to see an improvement in later models. 7.2 Model 1 Ranking by three years of SAT score. This is the best Bayesian model one can ever come up with. Just Kidding. As we discussed earlier in Chapter 6, Model 1 is overwhelmingly multicollinear; Model 1 is a sufficient sustitute if we want to look at the relationship between ranking and SAT score. 7.3 Model 2 2018 U.S. News Ranking by average SAT score and admissions rate of Year 2017. 7.3.1 First Impression There’s a strong positive relationship between the admission rate of colleges and college ranking (a larger number in college ranking is worse). From color gradient of points, we can see that schools with higher SAT averages tend to be better ranked. 7.3.2 Building the model 7.3.2.1 Step 1 A linear model provides some intuition about how the priors might be constructed. summary(lm(full_LiberalArts$Y2018 ~ full_LiberalArts$SAT_AVG_1617 + full_LiberalArts$ADM_RATE_1617)) ## ## Call: ## lm(formula = full_LiberalArts$Y2018 ~ full_LiberalArts$SAT_AVG_1617 + ## full_LiberalArts$ADM_RATE_1617) ## ## Residuals: ## Min 1Q Median 3Q Max ## -50.679 -9.613 -0.042 10.169 59.538 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 430.23894 34.40379 12.506 &lt;2e-16 *** ## full_LiberalArts$SAT_AVG_1617 -0.30402 0.02336 -13.012 &lt;2e-16 *** ## full_LiberalArts$ADM_RATE_1617 37.57722 13.59032 2.765 0.0068 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.48 on 98 degrees of freedom ## (48 observations deleted due to missingness) ## Multiple R-squared: 0.8577, Adjusted R-squared: 0.8548 ## F-statistic: 295.3 on 2 and 98 DF, p-value: &lt; 2.2e-16 7.3.2.2 Step 2 Next, we construct a hierarchical model of \\[Y_i = \\text{the predicted 2018 ranking of a liberal arts college}\\] by \\[\\begin{align} X_{1i} &amp; = \\text{student mean SAT score of Year 2016-17} \\\\ X_{2i} &amp; = \\text{admissions rate during Year 2016-17} \\end{align}\\] \\[\\begin{align} Y_i &amp; \\sim N(\\beta_{0i} + \\beta_{1i}X_{1i} + \\beta_{2i}X_{2i},\\tau_{\\text{big}[i]}) \\\\ \\beta_{0i} &amp; \\sim N(b_0,\\tau_0) \\\\ \\beta_{1i} &amp; \\sim N(b_1,\\tau_1) \\\\ \\beta_{2i} &amp; \\sim N(b_2,\\tau_2) \\\\ \\tau_{\\text{big}[i]} &amp; \\sim Gamma(s,r) \\\\ b_0 &amp; \\sim N(180,4000^{-1}) \\\\ \\tau_0 &amp; \\sim N(30, 1/9) \\\\ b_1 &amp; \\sim N(-0.1,0.001^{-1}) \\\\ \\tau_1 &amp; \\sim N(1000, 0.001^{-1}) \\\\ b_2 &amp; \\sim N(80,100^{-1}) \\\\ \\tau_2 &amp; \\sim N(10, 1) \\\\ s &amp; \\sim N(7,1) \\\\ r &amp; \\sim N(10000, 10000^{-1}) \\end{align}\\] liberal_model_2 &lt;- &quot;model{ # Data: observations for(i in 1:length(y)) { y[i] ~ dnorm(beta0[i] + beta1[i]*x1[i] + beta2[i]*x2[i], tau_big[i]) # Data: subjects beta0[i] ~ dnorm(b0, tau0) beta1[i] ~ dnorm(b1,tau1) beta2[i] ~ dnorm(b2,tau2) tau_big[i] ~ dgamma(s,r) } # Hyperpriors b0 ~ dnorm(180,1/4000) tau0 ~ dnorm(30, 1/9) b1 ~ dnorm(-0.1,1000) tau1 ~ dnorm(1000,1000) b2 ~ dnorm(80,1/100) tau2 ~ dnorm(10,1) s ~ dnorm(7,1) r ~ dnorm(10000, 1/10000) }&quot; # COMPILE y &lt;- full_LiberalArts$Y2018 model_data5 &lt;- as.data.frame(cbind(y, x1 = full_LiberalArts$SAT_AVG_1617, x2 = full_LiberalArts$ADM_RATE_1617)) model_data5 &lt;- na.omit(model_data5) liberal_jags_2 &lt;- jags.model(textConnection(liberal_model_2), data = list(y = model_data5$y, x1 = model_data5$x1, x2 = model_data5$x2), inits=list(.RNG.name = &quot;base::Wichmann-Hill&quot;, .RNG.seed = 454)) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 101 ## Unobserved stochastic nodes: 412 ## Total graph size: 1035 ## ## Initializing model # SIMULATE the model liberal_sim_2 &lt;- coda.samples(liberal_jags_2, variable.names = c(&quot;b0&quot;,&quot;tau0&quot;,&quot;b1&quot;,&quot;tau1&quot;,&quot;b2&quot;,&quot;tau2&quot;,&quot;s&quot;,&quot;r&quot;), n.iter = 10000) # STORE the chains in a data frame liberal_chains_2 &lt;- data.frame(liberal_sim_2[[1]]) 7.3.3 Model summary summary(liberal_sim_2) ## ## Iterations = 1001:11000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## b0 178.4834 0.56301 0.0056301 0.1895714 ## b1 -0.1090 0.02947 0.0002947 0.0039424 ## b2 43.1793 5.95589 0.0595589 4.0299101 ## r 10016.5430 99.18842 0.9918842 1.2203209 ## s 0.1517 0.01632 0.0001632 0.0009056 ## tau0 30.1001 3.00404 0.0300404 0.0510047 ## tau1 999.9994 0.03244 0.0003244 0.0004133 ## tau2 10.0017 1.01154 0.0101154 0.0176700 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## b0 177.2556 178.0642 178.5477 1.789e+02 1.793e+02 ## b1 -0.1706 -0.1287 -0.1086 -8.759e-02 -5.609e-02 ## b2 33.4737 38.3188 42.4722 4.779e+01 5.528e+01 ## r 9819.6765 9950.7472 10016.6172 1.008e+04 1.022e+04 ## s 0.1220 0.1402 0.1511 1.627e-01 1.850e-01 ## tau0 24.2727 28.0611 30.0777 3.214e+01 3.610e+01 ## tau1 999.9360 999.9776 999.9993 1.000e+03 1.000e+03 ## tau2 8.0292 9.3206 9.9837 1.067e+01 1.202e+01 7.3.4 Posterior inference For an unknown Liberal Arts College with student mean SAT score of 1450 and an admission rate of \\(30\\%\\) (e.g. Dvictor college), we could predict its ranking from our rjags simulation. liberal_chains_2 &lt;- liberal_chains_2 %&gt;% mutate(beta0_new = rnorm(10000,b0,(1/tau0)^(1/2))) %&gt;% mutate(beta1_new = rnorm(10000,b1,(1/tau1)^(1/2))) %&gt;% mutate(beta2_new = rnorm(10000,b2,(1/tau2)^(1/2))) %&gt;% mutate(tau_big_new = rgamma(10000,s,r)) %&gt;% mutate(ranking_new = rnorm(10000, mean = beta0_new + beta1_new * 1450 + beta2_new * 0.3, sd = (1/tau_big_new)^(1/2))) liberal_chains_2 %&gt;% summarize(quantile(ranking_new,0.025),quantile(ranking_new,0.975)) ## quantile(ranking_new, 0.025) quantile(ranking_new, 0.975) ## 1 -1869061 1816361 A \\(95\\%\\) credible interval is not pretty! This means that we over-evaluate the variability of a lot of predictors. 7.4 Future steps Given the problem that we encounter at Dvictor College, we also need to pay attention to narrowing the variability in our Bayesian models. (which really teaches us a lesson: simple is good; a complicated hierarchical model is not always helpful!) "],
["bayesian-models-part-2.html", "Chapter 8 Bayesian Models Part 2 8.1 Possible data cleaning? 8.2 On the way to Final Model 8.3 What are we thinking?", " Chapter 8 Bayesian Models Part 2 HINT: After you start SIMPLE, everything suddenly goes really complicated. After the last checkpoint, we realize that ranking is not the sole factor in students’ choice (sometimes not even an important factor). To accurately represent the school selection process, we find it more appropriate to create a new concept called Fitness Index that measures the level of fitness between the student and schools. We intend to create a Shiny App for students interested in finding colleges. Finding inputing predictive variables, the system will provide a list of top liberal arts colleges and universities. User Input in the Shiny App Description How users input Size of school (UGDS) number of undergraduate degree-seeking students Users can select an acceptable range of undergraduate student size between 0-4000 (for LACs) or 0-60000 (for Us). SAT Scores (SAT_AVG) combined SAT average by year, interchangeable with SAT Users can input their own SAT score. Our system will compare it with average SAT scores of each college and calculate students’ compatibility. (ACT score will be automatically transformed.) A college’s region (REGION) region-wise (e.g. Minnesota is in the Plains region) This is a decisive multi-checkbox. Users can filter off regions where they don’t want to attend college. Setting (LOCALE) setting (large city, small town, etc.) This is a low weight variable where students can state their preference of the geographical location of colleges. Colleges that fit their description will be considered more compatible with students; schools will not be eliminated solely by this factor. Average Cost Per Year (COSTT4_A) average cost of attendance per year Users can use a slidebar to set their preferred full tuition range. Racial diversity (UGDS_WHITE) racial diversity by the percentage of white students Users can select a preferred percentage. Schools with an incompatible percentage (&gt;15% net difference) will be punished by the algorithm; schools will only be eliminated solely by this factor if net difference &gt; 35%. 8.1 Possible data cleaning? “Zuofu was on his way to create the best Bayesian model in the world when his dream got crushed.” –Zuofu We were experimenting with our final model of rankings ~ predictors when we found out that more schools than we estimated had NA in their SAT score section (do not provide standardized test scores). We subsequently found that NAs exist throughout our dataset. Since our final goal is to create an interactive tool for students to evaluate their compatibility with colleges, we decide to leave these NAs alone until the final phase. The working plan is to re-weigh variables when building our fitness index algorithm. The weight of NA variables will be set to \\(0\\) by an if statement, and other variables will have a higher weight proportionally in correspondence. “Wait for it. We will come back later to deal with you!” –Zuofu again 8.2 On the way to Final Model This is when we realize not all factors play an essential role in the college ranking algorithm. To check our understanding, we build a quick linear regression model. From its summary and anova table, we recognize the important factors: size of undergraduate student, SAT score, admission rate, location and of course intercept. LinearModel &lt;- lm(as.numeric(Y2018) ~ UGDS_1617 + LOCALE_collapse + SAT_AVG_1617 + ADM_RATE_1617 + COSTT4_A_1617 + UGDS_WHITE_1617, fullUniversity) summary(LinearModel) ## ## Call: ## lm(formula = as.numeric(Y2018) ~ UGDS_1617 + LOCALE_collapse + ## SAT_AVG_1617 + ADM_RATE_1617 + COSTT4_A_1617 + UGDS_WHITE_1617, ## data = fullUniversity) ## ## Residuals: ## Min 1Q Median 3Q Max ## -91.881 -6.717 -1.486 9.020 40.251 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.014e+02 4.395e+01 4.584 1.26e-05 *** ## UGDS_1617 -6.681e-04 2.186e-04 -3.057 0.00283 ** ## LOCALE_collapseSuburb -3.084e+00 4.007e+00 -0.770 0.44318 ## LOCALE_collapseTown 6.109e+00 8.877e+00 0.688 0.49285 ## SAT_AVG_1617 -1.260e-01 3.035e-02 -4.150 6.74e-05 *** ## ADM_RATE_1617 8.617e+01 1.633e+01 5.276 7.04e-07 *** ## COSTT4_A_1617 -1.126e-04 1.540e-04 -0.731 0.46639 ## UGDS_WHITE_1617 2.020e+00 1.107e+01 0.183 0.85552 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.61 on 106 degrees of freedom ## (6 observations deleted due to missingness) ## Multiple R-squared: 0.814, Adjusted R-squared: 0.8017 ## F-statistic: 66.27 on 7 and 106 DF, p-value: &lt; 2.2e-16 anova(LinearModel) ## Analysis of Variance Table ## ## Response: as.numeric(Y2018) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## UGDS_1617 1 8699 8699 31.5151 1.600e-07 *** ## LOCALE_collapse 2 3730 1865 6.7556 0.001732 ** ## SAT_AVG_1617 1 104898 104898 380.0200 &lt; 2.2e-16 *** ## ADM_RATE_1617 1 10545 10545 38.2011 1.208e-08 *** ## COSTT4_A_1617 1 161 161 0.5816 0.447366 ## UGDS_WHITE_1617 1 9 9 0.0333 0.855516 ## Residuals 106 29259 276 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 OMG, what happened to the cost vairbale? Is it because I put the cost variable COSTT4_A_1617 in the last but two? In the spirit of scientific experiment (which Kavya and Zuofu happen to have), we switch the order of our linear model and produce another anova test: LinearModel2 &lt;- lm(as.numeric(Y2018) ~ UGDS_1617 + LOCALE_collapse + COSTT4_A_1617 + SAT_AVG_1617 + ADM_RATE_1617 + UGDS_WHITE_1617, fullUniversity) anova(LinearModel2) ## Analysis of Variance Table ## ## Response: as.numeric(Y2018) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## UGDS_1617 1 8699 8699 31.5151 1.600e-07 *** ## LOCALE_collapse 2 3730 1865 6.7556 0.001732 ** ## COSTT4_A_1617 1 39446 39446 142.9020 &lt; 2.2e-16 *** ## SAT_AVG_1617 1 66224 66224 239.9151 &lt; 2.2e-16 *** ## ADM_RATE_1617 1 9933 9933 35.9857 2.802e-08 *** ## UGDS_WHITE_1617 1 9 9 0.0333 0.855516 ## Residuals 106 29259 276 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Certainly, cost remains one of the most important factors for students in the process of college selection. We also acknowledge the significant role that scholarships and financial aids play. However, the order switch experiment sufficiently shows that other variables explain an equal amount of variability. 8.3 What are we thinking? From above, we made a final decision of incorporating the following variables into our Bayesian model of rankings: size of undergraduate student, SAT score, admission rate and location. To see how we do with universities and liberal arts colleges, check out our next chapter: Becoming a proud Bayesian. "],
["becoming-a-proud-bayesian.html", "Chapter 9 Becoming a proud Bayesian 9.1 University Model 9.2 Liberal Arts Colleges 9.3 Comparison and discussion", " Chapter 9 Becoming a proud Bayesian For our finalized Bayesian model, we intend to build predictive models for universities and liberal arts colleges. Then we’ll compare the similarities and differences of the two models. (i.e. is some factor important for universities and less so for liberal arts colleges?) 9.1 University Model 9.1.1 Get some intuition We could build bi-variate visualizations that help us understand the individual trends: 9.1.2 Some additional intuition For more intuition, we start with more vague priors and slightly adjust the priors based on the summary table. It’s important to note that the priors cannot be set too close to the means of the summary table in case of overfitting. 9.1.3 Building the model Our final university model predicts \\[Y_i = \\text{the predicted 2018 ranking of a university}\\] by \\[\\begin{align} X_{1,i} &amp; = \\text{the number of undergraduate students in university i} \\\\ Z_i &amp; = \\text{location of university i (City, Suburb, Town)} \\\\ X_{3,i} &amp; = \\text{student mean SAT score of Year 2016-17} \\\\ X_{3,i} &amp; = \\text{admissions rate during Year 2016-17} \\end{align}\\] Specifically: \\[\\begin{align} Y_i &amp; \\sim Pois(\\lambda_i) \\\\ log(\\lambda_i) &amp; = \\beta_0 + \\beta_1X_{1,i} + \\beta_{2,1}Z_{1,i} + \\beta_{2,2}Z_{2,i} + \\beta_{2,3}Z_{3,i} + \\beta_3X_{3,i} + \\beta_4X_{4,i} \\\\ \\beta_0 &amp; \\sim N(8,1) \\\\ \\beta_1 &amp; \\sim N(0,0.0001^{-1}) \\\\ \\beta_{2,1} &amp; = 0 \\\\ \\beta_{2,2} &amp; \\sim N(0,100^{-1}) \\\\ \\beta_{2,3} &amp; \\sim N(0,100^{-1}) \\\\ \\beta_{3} &amp; \\sim N(0,0.1^{-1}) \\\\ \\beta_{4} &amp; \\sim N(0,100^{-1}) \\end{align}\\] where \\(Z_{1,i}\\) indicates if a liberal arts college is in the city (reference level); \\(Z_{2,i}\\) indicates if a college is in the suburb setting; \\(Z_{3,i}\\) indicates if a college is in the town setting. university_model_3 &lt;- &quot;model{ # Data: observations for(i in 1:length(y)) { y[i] ~ dpois(lambda[i]) log(lambda[i]) = beta0 + beta1*x1[i] + beta2[z[i]] + beta3*x3[i] + beta4*x4[i] } # Data: subjects beta0 ~ dnorm(8, 1) beta1 ~ dnorm(0, 10000) beta2[1] &lt;- 0 beta2[2] ~ dnorm(0, 0.01) beta2[3] ~ dnorm(0, 0.01) beta3 ~ dnorm(0, 10) beta4 ~ dnorm(0, 0.01) }&quot; # COMPILE y &lt;- as.numeric(fullUniversity$Y2018) model_data9 &lt;- data.frame(y, x1 = fullUniversity$UGDS_1617, z = as.numeric(fullUniversity$LOCALE_collapse), x3 = fullUniversity$SAT_AVG_1617, x4 = fullUniversity$ADM_RATE_1617) model_data9 &lt;- na.omit(model_data9) university_jags_3 &lt;- jags.model(textConnection(university_model_3), data = list(y = model_data9$y, x1 = model_data9$x1, z = factor(model_data9$z), x3 = model_data9$x3, x4 = model_data9$x4), inits=list(.RNG.name = &quot;base::Wichmann-Hill&quot;, .RNG.seed = 454)) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 114 ## Unobserved stochastic nodes: 6 ## Total graph size: 1137 ## ## Initializing model # SIMULATE the model university_sim_3 &lt;- coda.samples(university_jags_3, variable.names = c(&quot;beta0&quot;,&quot;beta1&quot;,&quot;beta2&quot;,&quot;beta3&quot;,&quot;beta4&quot;), n.iter = 10000) # STORE the chains in a data frame university_chains_3 &lt;- data.frame(university_sim_3[[1]]) 9.1.4 Model summary summary(university_sim_3) ## ## Iterations = 1001:11000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## beta0 6.693e+00 3.148e-01 3.148e-03 1.064e-01 ## beta1 -4.418e-06 1.154e-06 1.154e-08 3.881e-08 ## beta2[1] 0.000e+00 0.000e+00 0.000e+00 0.000e+00 ## beta2[2] 1.529e-02 3.080e-02 3.080e-04 6.158e-04 ## beta2[3] 1.464e-01 5.940e-02 5.940e-04 1.345e-03 ## beta3 -2.539e-03 2.136e-04 2.136e-06 6.886e-05 ## beta4 1.374e+00 1.055e-01 1.055e-03 1.990e-02 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## beta0 6.1899756 6.474e+00 6.6367448 6.853e+00 7.416e+00 ## beta1 -0.0000067 -5.213e-06 -0.0000044 -3.621e-06 -2.184e-06 ## beta2[1] 0.0000000 0.000e+00 0.0000000 0.000e+00 0.000e+00 ## beta2[2] -0.0454972 -5.617e-03 0.0151296 3.629e-02 7.586e-02 ## beta2[3] 0.0265393 1.076e-01 0.1474028 1.870e-01 2.622e-01 ## beta3 -0.0030334 -2.650e-03 -0.0025023 -2.393e-03 -2.194e-03 ## beta4 1.1282817 1.311e+00 1.3846333 1.447e+00 1.551e+00 9.1.5 Posterior inference For an unknown university located in the city with 10000 undergraduates, student mean SAT score of 1400 and an admission rate of \\(25\\%\\) (e.g. Gvictor University), we could predict its ranking from our rjags simulation. university_chains_3 &lt;- university_chains_3 %&gt;% mutate(ranking_new = rpois(10000, lambda = exp(beta0 + beta1 * 10000 + beta3 * 1400 + beta4 * 0.25))) university_chains_3 %&gt;% summarize(quantile(ranking_new,0.025),quantile(ranking_new,0.975)) ## quantile(ranking_new, 0.025) quantile(ranking_new, 0.975) ## 1 21 43 The interval is more reflective of the intuitive estimate of Gvictor University’s ranking. 9.2 Liberal Arts Colleges 9.2.1 Get some intuition Similar to what we do with universities, we could build bi-variate visualizations that help us understand the individual trends: 9.2.2 Building the model Our final liberal arts college model predicts \\[Y_i = \\text{the predicted 2018 ranking of a liberal arts college}\\] by \\[\\begin{align} X_{1,i} &amp; = \\text{the number of undergraduate students in liberal arts college i} \\\\ Z_i &amp; = \\text{location of liberal arts college i (City, Rural, Suburb, Town)} \\\\ X_{3,i} &amp; = \\text{student mean SAT score of Year 2016-17} \\\\ X_{3,i} &amp; = \\text{admissions rate during Year 2016-17} \\end{align}\\] Specifically: \\[\\begin{align} Y_i &amp; \\sim Pois(\\lambda_i) \\\\ log(\\lambda_i) &amp; = \\beta_0 + \\beta_1X_{1,i} + \\beta_{2,1}Z_{1,i} + \\beta_{2,2}Z_{2,i} + \\beta_{2,3}Z_{3,i} + \\beta_{2,4}Z_{4,i} + \\beta_3X_{3,i} + \\beta_4X_{4,i} \\\\ \\beta_0 &amp; \\sim N(0,0.0001) \\\\ \\beta_1 &amp; \\sim N(0,0.0001^{-1}) \\\\ \\beta_{2,1} &amp;= 0 \\\\ \\beta_{2,2} &amp; \\sim N(0,25^{-1}) \\\\ \\beta_{2,3} &amp; \\sim N(-10,25^{-1}) \\\\ \\beta_{2,4} &amp; \\sim N(-15,100^{-1}) \\\\ \\beta_{3} &amp; \\sim N(-0.3,0.01^{-1}) \\\\ \\beta_{4} &amp; \\sim N(36,0.1^{-1}) \\end{align}\\] where \\(Z_{1,i}\\) indicates if a liberal arts college is in the city (reference level); \\(Z_{2,i}\\) indicates if a college is in the rural setting; \\(Z_{3,i}\\) indicates if a college is in the suburb setting; \\(Z_{4,i}\\) indicates if a college is in the town setting. liberal_model_3 &lt;- &quot;model{ # Data: observations for(i in 1:length(y)) { y[i] ~ dpois(lambda[i]) log(lambda[i]) = beta0 + beta1*x1[i] + beta2[z[i]] + beta3*x3[i] + beta4*x4[i] } # Data: subjects beta0 ~ dnorm(0, 0.0001) beta1 ~ dnorm(0, 10000) beta2[1] &lt;- 0 beta2[2] ~ dnorm(0, 0.04) beta2[3] ~ dnorm(-10, 0.04) beta2[4] ~ dnorm(-15, 0.01) beta3 ~ dnorm(-0.3, 100) beta4 ~ dnorm(36, 10) }&quot; # COMPILE y &lt;- as.numeric(full_LiberalArts$Y2018) model_data8 &lt;- data.frame(y, x1 = as.numeric(full_LiberalArts$UGDS_1617), z = as.numeric(full_LiberalArts$LOCALE_collapse_lac1), x3 = as.numeric(full_LiberalArts$SAT_AVG_1617), x4 = as.numeric(full_LiberalArts$ADM_RATE_1617)) model_data8 &lt;- na.omit(model_data8) liberal_jags_3 &lt;- jags.model(textConnection(liberal_model_3), data = list(y = model_data8$y, x1 = model_data8$x1, z = factor(model_data8$z), x3 = model_data8$x3, x4 = model_data8$x4), inits=list(.RNG.name = &quot;base::Wichmann-Hill&quot;, .RNG.seed = 454)) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 101 ## Unobserved stochastic nodes: 7 ## Total graph size: 1014 ## ## Initializing model # SIMULATE the model liberal_sim_3 &lt;- coda.samples(liberal_jags_3, variable.names = c(&quot;beta0&quot;,&quot;beta1&quot;,&quot;beta2&quot;,&quot;beta3&quot;,&quot;beta4&quot;), n.iter = 10000) # STORE the chains in a data frame liberal_chains_3 &lt;- data.frame(liberal_sim_3[[1]]) 9.2.3 Model summary summary(liberal_sim_3) ## ## Iterations = 1001:11000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## beta0 5.7575868 3.6590045 3.659e-02 9.521e-01 ## beta1 -0.0001047 0.0001043 1.043e-06 2.494e-05 ## beta2[1] 0.0000000 0.0000000 0.000e+00 0.000e+00 ## beta2[2] -0.1862067 0.1544641 1.545e-03 3.231e-02 ## beta2[3] -0.1221312 0.0503896 5.039e-04 6.979e-03 ## beta2[4] -0.5671952 0.2174377 2.174e-03 4.696e-02 ## beta3 -0.0024861 0.0028859 2.886e-05 7.252e-04 ## beta4 2.9260686 1.0288947 1.029e-02 2.916e-01 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## beta0 4.5244176 4.7629993 4.8721236 5.0651715 18.1141821 ## beta1 -0.0001654 -0.0001408 -0.0001265 -0.0001113 0.0002968 ## beta2[1] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 ## beta2[2] -0.2737200 -0.2348096 -0.2141429 -0.1912261 0.2485554 ## beta2[3] -0.1984605 -0.1453506 -0.1246138 -0.1044559 -0.0508085 ## beta2[4] -0.7104135 -0.6439770 -0.6077693 -0.5682324 0.1487860 ## beta3 -0.0119698 -0.0019412 -0.0018121 -0.0017256 -0.0015404 ## beta4 -0.9364295 3.1050759 3.1747504 3.2368810 3.3537106 9.2.4 Posterior inference For an unknown liberal arts college in a rural setting with 2000 undergraduates, student mean SAT score of 1400 and an admission rate of \\(25\\%\\) (e.g. Hvictor College), we could predict its ranking from our rjags simulation. liberal_chains_3 &lt;- liberal_chains_3 %&gt;% mutate(ranking_new = rpois(10000, exp(beta0 + beta1 * 2000 + beta2.4. + beta3 * 1400 + beta4 * 0.25))) liberal_chains_3 %&gt;% summarize(quantile(ranking_new,0.025),quantile(ranking_new,0.975)) ## quantile(ranking_new, 0.025) quantile(ranking_new, 0.975) ## 1 3 16 This is totally reasonable! The interval roughly reflects the intuitive estimate of Hvictor College’s ranking. 9.3 Comparison and discussion Compared to the last attempts, our final model gives reasonable predictions, which is a huge progress! However, the 95% credible interval it provides doesn’t provide an accurate enough interval as desired. By creating different imaginary universities and colleges, we are able to summarize the most important factors for universities and colleges respectively. We find that universities value SAT score as a more decisive indicator of both students’ admission and their own ranking. "],
["a-little-shiny.html", "Chapter 10 A Little Shiny 10.1 Shiny App 10.2 In case you’re curious", " Chapter 10 A Little Shiny Like we promised, students will be able to input their personal information and get a list of colleges and universities ranked top according to our fitness index algorithm. (In that way, Zuofu could pretend that there’s such person Zafo with a 1580 SAT and want to go to a big school in the city.) 10.1 Shiny App Considering the difficulty of embedding a Shiny App in the bookdown, we instead include links of our Shiny App that you could play around. Check out which colleges fit you the most! For liberal arts colleges you are interested in: https://zuofuhuang.shinyapps.io/collegeranking/ For universities you are interested in: https://zuofuhuang.shinyapps.io/UniversityRanking/ 10.2 In case you’re curious If you are curious about the code (which is somehow complicated), here it is! Email Zuofu for more details. 10.2.1 Code for Shiny App “collegeranking” server4 &lt;- function(input, output) { output$output_table &lt;- renderTable({ datasize &lt;- req(input$schoolsize) lowersize &lt;- datasize[1] uppersize &lt;- datasize[2] full_LiberalArts1 &lt;- full_LiberalArts %&gt;% filter(UGDS_1617 &gt; lowersize) %&gt;% filter(UGDS_1617 &lt; uppersize) datasat &lt;- req(input$sat) if (datasat &gt; 36){ datasat = datasat } else { datasat = datasat * 40 + 160 } satvector &lt;- rep(0,147) for (i in 1:147){ if (is.na(full_LiberalArts[i,]$SAT_AVG_1617) == TRUE){ satvector[i] &lt;- NA } else if (datasat - full_LiberalArts[i,]$SAT_AVG_1617 &gt; 0 &amp; datasat - full_LiberalArts[i,]$SAT_AVG_1617 &lt;= 130){ satvector[i] &lt;- datasat - full_LiberalArts[i,]$SAT_AVG_1617 } else if (datasat - full_LiberalArts[i,]$SAT_AVG_1617 &gt; 130){ satvector[i] &lt;- 130 - (datasat - full_LiberalArts[i,]$SAT_AVG_1617)*0.5 } else { satvector[i] &lt;- datasat - full_LiberalArts[i,]$SAT_AVG_1617 } } dataregion &lt;- input$region if (&quot;NewEngland&quot; %in% dataregion){ full_LiberalArts1 &lt;- full_LiberalArts1 %&gt;% filter(REGION_collapse != &quot;NewEngland&quot;) } if (&quot;GreatLakes&quot; %in% dataregion){ full_LiberalArts1 &lt;- full_LiberalArts1 %&gt;% filter(REGION_collapse != &quot;GreatLakes&quot;) } if (&quot;Plains&quot; %in% dataregion){ full_LiberalArts1 &lt;- full_LiberalArts1 %&gt;% filter(REGION_collapse != &quot;Plains&quot;) } if (&quot;Southeast&quot; %in% dataregion){ full_LiberalArts1 &lt;- full_LiberalArts1 %&gt;% filter(REGION_collapse != &quot;Southeast&quot;) } if (&quot;Southwest&quot; %in% dataregion){ full_LiberalArts1 &lt;- full_LiberalArts1 %&gt;% filter(REGION_collapse != &quot;Southwest&quot;) } if (&quot;RockyMountains&quot; %in% dataregion){ full_LiberalArts1 &lt;- full_LiberalArts1 %&gt;% filter(REGION_collapse != &quot;RockyMountains&quot;) } if (&quot;Farwest&quot; %in% dataregion){ full_LiberalArts1 &lt;- full_LiberalArts1 %&gt;% filter(REGION_collapse != &quot;Farwest&quot;) } if (&quot;NA&quot; %in% dataregion){ full_LiberalArts1 &lt;- full_LiberalArts1 %&gt;% filter(REGION_collapse != &quot;NA&quot;) } if (&quot;MidEast&quot; %in% dataregion){ full_LiberalArts1 &lt;- full_LiberalArts1 %&gt;% filter(REGION_collapse != &quot;MidEast&quot;) } datalocation &lt;- input$location if (datalocation == &quot;City&quot;){ datalocation &lt;- 3 } else if (datalocation == &quot;Suburb&quot;){ datalocation &lt;- 2 } else if (datalocation == &quot;Town&quot;){ datalocation &lt;- 1 } else { datalocation &lt;- 0 } locationvector &lt;- rep(NA,147) for (i in 1:147){ locationvector[i] &lt;- 15 * abs(datalocation - full_LiberalArts[i,]$LOCALE_collapse_lac3) } datacost &lt;- input$cost lowercost &lt;- datacost[1] uppercost &lt;- datacost[2] full_LiberalArts2 &lt;- full_LiberalArts1 %&gt;% filter(COSTT4_A_1617 &gt; lowercost) %&gt;% filter(COSTT4_A_1617 &lt; uppercost) datadiversity &lt;- input$diversity datadiversitylow &lt;- datadiversity - 0.15 datadiversityhigh &lt;- datadiversity + 0.15 datadiversitylower &lt;- datadiversity - 0.35 datadiversityhigher &lt;- datadiversity + 0.35 full_LiberalArts3 &lt;- full_LiberalArts2 %&gt;% filter(UGDS_WHITE_1617 &lt; datadiversityhigher) %&gt;% filter(UGDS_WHITE_1617 &gt; datadiversitylower) diversityvector &lt;- rep(NA,147) for (i in 1:147){ if (is.na(full_LiberalArts[i,]$UGDS_WHITE_1617) == TRUE){ diversityvector[i] &lt;- NA } else if (full_LiberalArts[i,]$UGDS_WHITE_1617 &gt; datadiversityhigh){ diversityvector[i] &lt;- -500*abs(full_LiberalArts[i,]$UGDS_WHITE_1617 - datadiversityhigh) } else if (full_LiberalArts[i,]$UGDS_WHITE_1617 &lt; datadiversitylow){ diversityvector[i] &lt;- -500*abs(full_LiberalArts[i,]$UGDS_WHITE_1617 - datadiversitylow) } else { diversityvector[i] &lt;- 0 } } fitnessvector &lt;- rep(0,147) for (j in 1:147){ if (is.na(satvector[j]) == TRUE &amp; is.na(locationvector[j]) == TRUE &amp; is.na(diversityvector[j]) == TRUE){ fitnessvector[j] &lt;- 0 } if (is.na(satvector[j]) == FALSE &amp; is.na(locationvector[j]) == TRUE &amp; is.na(diversityvector[j]) == TRUE){ fitnessvector[j] &lt;- satvector[j] } if (is.na(satvector[j]) == TRUE &amp; is.na(locationvector[j]) == FALSE &amp; is.na(diversityvector[j]) == TRUE){ fitnessvector[j] &lt;- locationvector[j] } if (is.na(satvector[j]) == TRUE &amp; is.na(locationvector[j]) == TRUE &amp; is.na(diversityvector[j]) == FALSE){ fitnessvector[j] &lt;- diversityvector[j] } if (is.na(satvector[j]) == FALSE &amp; is.na(locationvector[j]) == FALSE &amp; is.na(diversityvector[j]) == TRUE){ fitnessvector[j] &lt;- (5/8) * satvector[j] + (3/8) * locationvector[j] } if (is.na(satvector[j]) == TRUE &amp; is.na(locationvector[j]) == FALSE &amp; is.na(diversityvector[j]) == FALSE){ fitnessvector[j] &lt;- 0.6 * locationvector[j] + 0.4 * diversityvector[j] } if (is.na(satvector[j]) == FALSE &amp; is.na(locationvector[j]) == TRUE &amp; is.na(diversityvector[j]) == FALSE){ fitnessvector[j] &lt;- (5/7) * satvector[j] + (2/7) * diversityvector[j] } if (is.na(satvector[j]) == FALSE &amp; is.na(locationvector[j]) == FALSE &amp; is.na(diversityvector[j]) == FALSE){ fitnessvector[j] &lt;- 0.5 * satvector[j] + 0.3 * locationvector[j] + 0.2 * diversityvector[j] } } fitnessmatrixA &lt;- data.frame(cbind(fitnessvector,full_LiberalArts$CollegeName)) fitnessmatrixA &lt;- fitnessmatrixA %&gt;% mutate(fitnessvector = as.numeric(as.character(fitnessvector))) fittingcolleges &lt;- rep(NA,dim(full_LiberalArts3)[1]) for (i in 1:dim(full_LiberalArts3)[1]){ fittingcolleges[i] = full_LiberalArts3[i,]$CollegeName } fittingcolleges &lt;- data.frame(fittingcolleges) fitnessmatrixB &lt;- fitnessmatrixA %&gt;% semi_join(fittingcolleges, by = c(&quot;V2&quot; = &quot;fittingcolleges&quot;)) fitnessmatrixB &lt;- fitnessmatrixB %&gt;% mutate(V2 = as.character(V2)) fitnessmatrixB &lt;- fitnessmatrixB[order(-fitnessmatrixB$fitnessvector),] fiveschools &lt;- c(fitnessmatrixB[1,2],fitnessmatrixB[2,2],fitnessmatrixB[3,2],fitnessmatrixB[4,2],fitnessmatrixB[5,2]) fiveschoolindex &lt;- c(fitnessmatrixB[1,1],fitnessmatrixB[2,1],fitnessmatrixB[3,1],fitnessmatrixB[4,1],fitnessmatrixB[5,1]) output_table &lt;- data.table(fiveschools,fiveschoolindex) output_table }) } #build the user interface ui4 &lt;- fluidPage( headerPanel(&quot;Choose your inputs&quot;), sidebarLayout( sidebarPanel( sliderInput(&quot;schoolsize&quot;,label = &quot;Acceptable range of school size:&quot;, min = 0, max = 4000, step = 50, value = c(0,4000)), numericInput(&quot;sat&quot;, label = &quot;Your standardized test score:&quot;, min = 0, max = 1600, value = 0, step = NA), selectInput(&quot;region&quot;, label = &quot;Select **off** regions:&quot;, choices = c(&quot;NewEngland&quot;,&quot;MidEast&quot;,&quot;GreatLakes&quot;,&quot;Plains&quot;,&quot;Southeast&quot;,&quot;Southwest&quot;,&quot;RockyMountains&quot;,&quot;FarWest&quot;,&quot;NA&quot;), multiple = TRUE), selectInput(&quot;location&quot;, label = &quot;Geographical Location:&quot;, choices = c(&quot;City&quot;,&quot;Suburb&quot;,&quot;Town&quot;,&quot;Rural&quot;)), sliderInput(&quot;cost&quot;, label = &quot;Ideal cost/yr range excluding scholarships:&quot;, min = 0, max = 100000, step = 100, value = c(0,100000)), numericInput(&quot;diversity&quot;, label = &quot;Input your ideal school racial diversity:&quot;, min = 0, max = 1, value = 0, step = 0.01) ), mainPanel( h4(&quot;List of Schools&quot;), tableOutput(&quot;output_table&quot;) ) ) ) shinyApp(ui = ui4, server = server4) 10.2.2 Code for Shiny App “UniversityRanking” server5 &lt;- function(input, output) { output$output_table &lt;- renderTable({ datasize &lt;- req(input$schoolsize) lowersize &lt;- datasize[1] uppersize &lt;- datasize[2] fullUniversity1 &lt;- fullUniversity %&gt;% filter(UGDS_1617 &gt; lowersize) %&gt;% filter(UGDS_1617 &lt; uppersize) datasat &lt;- req(input$sat) if (datasat &gt; 36){ datasat = datasat } else { datasat = datasat * 40 + 160 } satvector &lt;- rep(0,120) for (i in 1:120){ if (is.na(fullUniversity[i,]$SAT_AVG_1617) == TRUE){ satvector[i] &lt;- NA } else if (datasat - fullUniversity[i,]$SAT_AVG_1617 &gt; 0 &amp; datasat - fullUniversity[i,]$SAT_AVG_1617 &lt;= 130){ satvector[i] &lt;- datasat - fullUniversity[i,]$SAT_AVG_1617 } else if (datasat - fullUniversity[i,]$SAT_AVG_1617 &gt; 130){ satvector[i] &lt;- 130 - (datasat - fullUniversity[i,]$SAT_AVG_1617)*0.5 } else { satvector[i] &lt;- datasat - fullUniversity[i,]$SAT_AVG_1617 } } dataregion &lt;- input$region if (&quot;NewEngland&quot; %in% dataregion){ fullUniversity1 &lt;- fullUniversity1 %&gt;% filter(REGION_collapse != &quot;NewEngland&quot;) } if (&quot;GreatLakes&quot; %in% dataregion){ fullUniversity1 &lt;- fullUniversity1 %&gt;% filter(REGION_collapse != &quot;GreatLakes&quot;) } if (&quot;Plains&quot; %in% dataregion){ fullUniversity1 &lt;- fullUniversity1 %&gt;% filter(REGION_collapse != &quot;Plains&quot;) } if (&quot;Southeast&quot; %in% dataregion){ fullUniversity1 &lt;- fullUniversity1 %&gt;% filter(REGION_collapse != &quot;Southeast&quot;) } if (&quot;Southwest&quot; %in% dataregion){ fullUniversity1 &lt;- fullUniversity1 %&gt;% filter(REGION_collapse != &quot;Southwest&quot;) } if (&quot;RockyMountains&quot; %in% dataregion){ fullUniversity1 &lt;- fullUniversity1 %&gt;% filter(REGION_collapse != &quot;RockyMountains&quot;) } if (&quot;Farwest&quot; %in% dataregion){ fullUniversity1 &lt;- fullUniversity1 %&gt;% filter(REGION_collapse != &quot;Farwest&quot;) } if (&quot;MidEast&quot; %in% dataregion){ fullUniversity1 &lt;- fullUniversity1 %&gt;% filter(REGION_collapse != &quot;MidEast&quot;) } datalocation &lt;- input$location if (datalocation == &quot;City&quot;){ datalocation &lt;- 3 } else if (datalocation == &quot;Suburb&quot;){ datalocation &lt;- 2 } else if (datalocation == &quot;Town&quot;){ datalocation &lt;- 1 } else { datalocation &lt;- 0 } locationvector &lt;- rep(NA,120) for (i in 1:120){ locationvector[i] &lt;- 15 * abs(datalocation - fullUniversity[i,]$LOCALE_collapse3) } datacost &lt;- input$cost lowercost &lt;- datacost[1] uppercost &lt;- datacost[2] fullUniversity2 &lt;- fullUniversity1 %&gt;% filter(COSTT4_A_1617 &gt; lowercost) %&gt;% filter(COSTT4_A_1617 &lt; uppercost) datadiversity &lt;- input$diversity datadiversitylow &lt;- datadiversity - 0.15 datadiversityhigh &lt;- datadiversity + 0.15 datadiversitylower &lt;- datadiversity - 0.35 datadiversityhigher &lt;- datadiversity + 0.35 fullUniversity3 &lt;- fullUniversity2 %&gt;% filter(UGDS_WHITE_1617 &lt; datadiversityhigher) %&gt;% filter(UGDS_WHITE_1617 &gt; datadiversitylower) diversityvector &lt;- rep(NA,120) for (i in 1:120){ if (is.na(fullUniversity[i,]$UGDS_WHITE_1617) == TRUE){ diversityvector[i] &lt;- NA } else if (fullUniversity[i,]$UGDS_WHITE_1617 &gt; datadiversityhigh){ diversityvector[i] &lt;- -500*abs(fullUniversity[i,]$UGDS_WHITE_1617 - datadiversityhigh) } else if (fullUniversity[i,]$UGDS_WHITE_1617 &lt; datadiversitylow){ diversityvector[i] &lt;- -500*abs(fullUniversity[i,]$UGDS_WHITE_1617 - datadiversitylow) } else { diversityvector[i] &lt;- 0 } } fitnessvector &lt;- rep(0,120) for (j in 1:120){ if (is.na(satvector[j]) == TRUE &amp; is.na(locationvector[j]) == TRUE &amp; is.na(diversityvector[j]) == TRUE){ fitnessvector[j] &lt;- 0 } if (is.na(satvector[j]) == FALSE &amp; is.na(locationvector[j]) == TRUE &amp; is.na(diversityvector[j]) == TRUE){ fitnessvector[j] &lt;- satvector[j] } if (is.na(satvector[j]) == TRUE &amp; is.na(locationvector[j]) == FALSE &amp; is.na(diversityvector[j]) == TRUE){ fitnessvector[j] &lt;- locationvector[j] } if (is.na(satvector[j]) == TRUE &amp; is.na(locationvector[j]) == TRUE &amp; is.na(diversityvector[j]) == FALSE){ fitnessvector[j] &lt;- diversityvector[j] } if (is.na(satvector[j]) == FALSE &amp; is.na(locationvector[j]) == FALSE &amp; is.na(diversityvector[j]) == TRUE){ fitnessvector[j] &lt;- (5/8) * satvector[j] + (3/8) * locationvector[j] } if (is.na(satvector[j]) == TRUE &amp; is.na(locationvector[j]) == FALSE &amp; is.na(diversityvector[j]) == FALSE){ fitnessvector[j] &lt;- 0.6 * locationvector[j] + 0.4 * diversityvector[j] } if (is.na(satvector[j]) == FALSE &amp; is.na(locationvector[j]) == TRUE &amp; is.na(diversityvector[j]) == FALSE){ fitnessvector[j] &lt;- (5/7) * satvector[j] + (2/7) * diversityvector[j] } if (is.na(satvector[j]) == FALSE &amp; is.na(locationvector[j]) == FALSE &amp; is.na(diversityvector[j]) == FALSE){ fitnessvector[j] &lt;- 0.5 * satvector[j] + 0.3 * locationvector[j] + 0.2 * diversityvector[j] } } fitnessmatrixA &lt;- data.frame(cbind(fitnessvector,fullUniversity$UniversityName)) fitnessmatrixA &lt;- fitnessmatrixA %&gt;% mutate(fitnessvector = as.numeric(as.character(fitnessvector))) fittinguniversities &lt;- rep(NA,dim(fullUniversity3)[1]) for (i in 1:dim(fullUniversity3)[1]){ fittinguniversities[i] = fullUniversity3[i,]$UniversityName } fittinguniversities &lt;- data.frame(fittinguniversities) fitnessmatrixB &lt;- fitnessmatrixA %&gt;% semi_join(fittinguniversities, by = c(&quot;V2&quot; = &quot;fittinguniversities&quot;)) fitnessmatrixB &lt;- fitnessmatrixB %&gt;% mutate(V2 = as.character(V2)) fitnessmatrixB &lt;- fitnessmatrixB[order(-fitnessmatrixB$fitnessvector),] fiveschools &lt;- c(fitnessmatrixB[1,2],fitnessmatrixB[2,2],fitnessmatrixB[3,2],fitnessmatrixB[4,2],fitnessmatrixB[5,2]) fiveschoolindex &lt;- c(fitnessmatrixB[1,1],fitnessmatrixB[2,1],fitnessmatrixB[3,1],fitnessmatrixB[4,1],fitnessmatrixB[5,1]) output_table &lt;- data.table(fiveschools,fiveschoolindex) output_table }) } #build the user interface ui5 &lt;- fluidPage( headerPanel(&quot;Choose your inputs&quot;), sidebarLayout( sidebarPanel( sliderInput(&quot;schoolsize&quot;,label = &quot;Acceptable range of school size:&quot;, min = 0, max = 60000, step = 500, value = c(0,60000)), numericInput(&quot;sat&quot;, label = &quot;Your standardized test score:&quot;, min = 0, max = 1600, value = 0, step = NA), selectInput(&quot;region&quot;, label = &quot;Select **off** regions:&quot;, choices = c(&quot;NewEngland&quot;,&quot;MidEast&quot;,&quot;GreatLakes&quot;,&quot;Plains&quot;,&quot;Southeast&quot;,&quot;Southwest&quot;,&quot;RockyMountains&quot;,&quot;FarWest&quot;), multiple = TRUE), selectInput(&quot;location&quot;, label = &quot;Geographical Location:&quot;, choices = c(&quot;City&quot;,&quot;Suburb&quot;,&quot;Town&quot;,&quot;Rural&quot;)), sliderInput(&quot;cost&quot;, label = &quot;Ideal cost/yr range excluding scholarships:&quot;, min = 0, max = 100000, step = 100, value = c(0,100000)), numericInput(&quot;diversity&quot;, label = &quot;Input your ideal school racial diversity:&quot;, min = 0, max = 1, value = 0, step = 0.01) ), mainPanel( h4(&quot;List of Schools&quot;), tableOutput(&quot;output_table&quot;) ) ) ) shinyApp(ui = ui5, server = server5) "]
]
